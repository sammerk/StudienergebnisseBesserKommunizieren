---
title: Evidenz.Besser.Kommunizieren.
subtitle: Wie Bildungswissenschaften und Fachdidaktiken ihre Wissenschaftskommunikation weiterentwickeln können.
shorttitle: Evidenz.Besser.Kommunizieren.
author:
  - name: Samuel Merk
    corresponding: true
    orcid: 0000-0003-2594-5337
    email: merk@ph-karlsruhe.de
    roles:
      - conceptualization
      - data curation
      - formal Analysis
      - investigation
      - methodology
      - software
      - supervision
      - validation
      - visualization
      - writing inital draft
      - editing
    affiliations:
      - id: id1
        name: "Pädagogische Hochschule Karlsruhe"
        address: Bismarckstraße 10
        city: Karlsruhe
        country: Germany
        postal-code: 76133
  - name: Sarah Bez
    affiliations:
      - id: id1
  - name: Kirstin Schmidt
    affiliations:
      - id: id1
author-note:
  disclosures:
    data-sharing: Die Daten dieses Artikels sind zu finden unter
    conflict-of-interest: Die Autor\:innen haben keine Interessenskonflikte zu berichten
abstract: "Lehrkräfte treffen tagtäglich unzählige Entscheidungen bzgl. ihrer Unterrichtsgestaltung und -entwicklung<!-- SB: Sonst eher immer Schul- und Unterrichtsentwicklung genannt und daher hier evtl auch?-->. Dabei rekurrieren sie vornehmlich auf persönliche Erfahrung, Konzeptwissen oder Heuristiken. Evidenz aus Bildungswissenschaften und Fachdidaktiken wird das Potenzial zugeschrieben, diese Entscheidungsprozesse komplementär<!-- SB: komplementär ist nach meiner Auffassung eher dichotom im Sinne von gegensätzlich, vielleicht eher: ergänzend? Dann weniger gegensätzlich, aber nur, wenn man das auch so meint :-)--> zu informieren und zu objektivieren. Dazu ist es jedoch notwendig, dass die betroffenen Lehrkräfte diese Evidenz nicht fehlinterpretieren, was wiederum entsprechende Kompetenzen der Lehrkräfte oder besonders geschickte <!-- gelungene?--> Wissenschaftkommunikation voraussetzt. Der vorliegende Beitrag untersucht daher die Möglichkeiten und Grenzen der Kommunikation von Effektstärken an Lehramtsstudierende am Beispiel des sog. zweiten PISA-Schocks<!-- SB: diese Formulierung kommt später nicht mehr und wird daher auch nicht weiter eingeordnet. Würde daher hier vielleicht eher schreiben: am Beispiel der letzten PISA-Ergebnisse oder so-->. Im Ergebnis zeigt sich, dass Lehramtsstudierende Effektstärken sehr ungenau (Noise) ein- und im Mittel drastisch überschätzen (Practical Significance Bias). Dieser Bias konnte durch die Verwendung alternativer Visualisierungen deutlich eingedämmt<!-- SB: finde diese Formulierung eher wertend und relativ stark und zudem imppliziert eingedämmt für mich sozusagen Feuer unter Kontrolle und quasi gelöscht, aber das zeigt die Studie ja grade leider nicht:-/ vielleicht eher: verringert oder vermindert?-->  werden $(d = .5)$"
# Put as many keywords at you like, separated by commmas (e.g., [reliability, validity, generalizability])
keywords: [Lehrpersonenprofessionalisierung, Wissenschaftskommunikation, Practical Significance Bias]
# If true, tables and figures are mingled with the text instead of listed at the end of the document.
floatsintext: true
# Numbered lines (.pdf and .docx only)
numbered-lines: false
# File with references
bibliography: references.bib
# Suppress title page
suppress-title-page: false
# Link citations to references
link-citations: true
notebook-links: true
lang: de
---

<!-- Max. 35k inklusive allem-->
<!-- SB: noch zu den Keywords: hätte hier noch evidence-informed practice o.ä. erwartet. Würde mich interessieren, warum Du das bewusst weggelassen hast?--> 
<!-- SB: noch zum Abstract: Würde ggf. noch 1-2 Sätze zur Diskussion bzw. den 3 Implikationen ergänzen. So wirkt der Abstract für mich etwas unvollständig.
Und noch ein anderer Punkt, der mir aufgefallen ist: Im ergebnisteil steht als ES Cliffs d = .23 drin, hier im Abstract d = .5. Selbst wenn mit letzterem Cohens d gemeint sein sollte, entspräche das nach https://cliffdelta.shinyapps.io/calculator/ einem cliffs delta von .33. (?)--> 
Die bildungswissenschaftliche Literatur zu Schul- und Unterrichtsentwicklung bedient sich einer Vielzahl theoretischer Grundlegungen [@bohl2020] und blickt daher aus ganz verschiedenen Winkeln auf diesen Gegenstand: Neben eher systemtheoretischen Perspektiven [@bauer1978] finden sich u.a. Ansätze mit Entlehnungen aus der Lehr-Lern- [@helmke2022] und Organisationspsychologie [@holtappels2007] oder mit dem Leitgedanken der Praxisorientierung [@bruegelmann2018]. Datenbasierte Schulentwicklung <!-- SB: siehe Abstract: eher Datenbasierte Schul- und Unterrichtsentwicklung?--> hat im deutschsprachigen Raum erst in den vergangenen zwei Dekaden Momentum gefunden<!-- SB: Momentum gefunden ist für mich keine geläufige Formulierung. Vielleicht: ist aufgekommen; hat Eingang gefunden o.ä.--> , wenngleich deren Grundidee des empirischen Einholens von Information über den Ist-Stand schon zuvor gefordert und auch umgesetzt wurde [@altrichter2006]. In jüngerer Zeit ist jedoch von inner- wie außerwissenschaftlichen Stakeholdern vermehrt die Forderung nach einer Entwicklung von Schule und Unterricht hörbar geworden, die ihre Entscheidungen durch Evidenz informiert [@aero2023; @bauer2012; @eurlex2024; @pellegrini2021; @slavin2020]. Da jedoch einerseits die Genese und Interpretation von Evidenz nicht zu den professionellen Kernkompetenzen von Lehrkäften gehört und andererseits Bildungswissenschaftler- und Fachdidaktiker:innen keine Expert:innen für die Gestaltung von Schule und Unterricht sind, plädiert der vorliegende Beitrag dafür, Wissenschaftskommunikation erstens als dialogischen Prozess zwischen Bildungswissenschaften/Fachdidaktiken und Lehrkräften aufzufassen und zweitens diesen forschungsbasiert weiterzuentwickeln.<!-- SB: Würde hier mit BLick auf Konsistenz/Kohäsion alle drei Implikationen und diese auch in derselben Reihenfolge wie in der Diskussion anbringen.--> 

Daher führt der folgende theoretische Hintergrund zunächst in Konzepte und Begriffe evidenzinformierter Praxis ein, bevor er auf Wissenschaftskommunikation in Bildungswissenschaften und Fachdidaktiken eingeht, um abschließend ein empirisches Beispiel zu skizzieren.

# Theoretischer Hintergrund

## Evidenzinformiertes Handeln

### Was kann unter »Evidenz« verstanden werden?

Etymologisch kann »Evidenz« als Substantivierung des Adjektivs »evident« gesehen werden [@kluge2011, S.263], welches wiederum im 18. Jahrhundert dem lateinischen »evidens« [»ersichtlich, augenscheinlich«, @hau2012] entlehnt wurde [@stark2017]. Allerdings meinen Bildungswissenschaftler:innen und Fachdidaktiker:innen gerade nicht »das Augenscheinliche« oder »das direkt Ersichtliche«, wenn sie von Evidenz sprechen - vielmehr ist in Definitionsvorschlägen von »wissenschaftlichem Wissen« [@stark2017], von einer »Funktion« von Daten für die Bestätigung oder Widerlegung von Hypothesen und Theorien [@bromme2014b] oder von »warrants for making assertions or knowledge claims« [@shavelson2002] die Rede. In einer aktuellen Systematisierung verschiedener Verständnisse des Evidenz-Begriffs in den Bildungswissenschaften hebt Schmidt [-@schmidt2024] hervor, dass nur wenige Definitionen ausschließlich quantitativer Empirie die Möglichkeit zuschreiben, Evidenz zu generieren, sondern meistens auch qualitative Empirie, Theorien sowie mathematische und logische Analysen als potenziell evidenzgenerierend definiert werden. Insbesondere die Inklusion nicht-empirischer Entitäten wie »Theorien« oder »logische Analysen« mögen auf den ersten Blick widersprüchlich wirken, da der Begriff Evidenz insbesondere im deutschsprachigen Raum teils mit Ergebnissen explanativer quantitativer Studien assoziiert scheint<!-- SB: Das ist zwar vorsichtig formuliert, aber ein, zwei Belege/Beispiele für diesen Claim einzufügen, wäre überlegenswert.-->. Dieser scheinbare Widerspruch wirkt jedoch weniger stark, berücksichtigt man, dass insbesondere in der Lehr-Lernforschung mit »Theorien« wohl eher sogenannte »tried-and-tested theories« [@renkl2022] gemeint sein dürften. Diese stellen eher Rahmenmodelle oder sogenannte »interventional models« (z.B. Cognitive Theory of Multi-Media Learning) dar (ebd.). Da solche »Theorien« wiederum meist stark von empirischen Ergebnissen beeinflusst sind. Daher ist es plausibel, ihnen die Funktion als »warrant« für »knowledge claims« zuzuschreiben und sie also auch als »Evidenz« zu bezeichnen.

### Evidenzinformiert, evidenzorientiert, evidenzbasiert.
<!-- SB: warum hier Punkt am Ende?-->
Im vorigen Abschnitt wurde deutlich, dass »Evidenz« ein uneinheitlich gebrauchter und gleichermaßen komplex wie unscharf definierter Begriff ist. Im Lichte dessen erscheint es nur konsequent, dass auch die Begriffe evidenzbasiert, evidenzinformiert, evidenzorientiert, datenbasiert, forschungsbasiert und forschungsinformiert klassisches *Jingle and Jangle* [@thorndike1904; @kelly2023] darstellen.<!-- SB: vielleicht eher: als Jingle and Jangle einzuordnen sind; finde die bisherige Formulierung eher untypisch--> D.h., dass also unterschiedliche Begriffe für das Gleiche und gleiche Begriffe für Unterschiedliches gebraucht werden. Die Differenzen zwischen evidenz**basiert** und evidenz**informiert** sowie evidenz**orientiert** können jedoch auch bedeutsam interpretiert werden: Da mit »Evidenz**basierung**« oft »the medical model« [@jones2024] und damit Evidenz aus *Kontrollgruppenexperimenten* als *notwendige Voraussetzung* für eine Entscheidung assoziiert wird, zieht dieser Begriff die stärkste Kritik auf sich [@bellmann2011; @biesta2007]. Den Begriffen »evidenz**orientiert**« und »evidenz**informiert**« wird mit weniger Fundamentalkritik begegnet, da diese schon rein sprachlich eher eine heuristische denn eine rechenschaftslegende Rolle implizieren.<!-- @Sarah: Ist der Satz so verständlich? SB: nein, ich bin in diesem Absatz und auch beim letzten Satz nicht ganz mitgekommen. Ich versuche, das zu formulieren: 
1.) Oben werden nicht nur evidenzinformiert/orientiert/basiert sondern auch daten..., forschungs... aufgeführt, aber dann nicht mehr darauf eingegangen. Hier werden die Lesenden im Unklaren gelassen. 
2.) Die Kriterien von ei, eo und eb für die Unterscheidung werden anhand der Formulierungen nicht klar: evidenzbasiert wird inhaltlich "definiert" (KGExperimente), worauf sich die Kritik am Begriff bezieht. evidenzorientiert und evidenzinformiert werden nur anhand der weniger starken Kritik abgegrenzt (also auf die Ebene der Rezeption/Kritik angesprochen). Was mit "heuristischer und rechenschafslegender Rolle" (also einem Kriterium, das nicht auf der Ebene der Forschungsdesigns (KGexperimente) oder der Kritik sondern auf der Ebene der Nutzung angesiedelt ist) gemeint ist, bleibt unscharf. Und ebenso bleibt im ungefähren, ob evidenzorientiert und evidenzinformiert hier synonym sind oder nicht.-->

<!-- SB: Und noch was Formales: Ich habe nicht ganz verstanden, wann Begriffe kursiv und wann mit »« und wann beides eingesetzt wird. Ich habe nichts geändert, weil das ja auch von den Vorgaben des Bandes abhängen, aber mir ist es nicht klar geworden und hat es zwischendurch eher verwirrt (warum ist z.B. Cognitive Load Theory in plain? wann sind »« als direkte Zitate gemeint und wann als begriffshervorhebung usw.)? Kann da gerne ggf. nochmal einen entsprechenden Durchgang machen.  -->

In der deutschsprachigen bildungswissenschaftlichen Diskussion sind nach Bromme et al.[-@bromme2014b] zunächst zwei verschiedene Diskussionsstränge bzgl. evidenzinformierter Entscheidungen im Bildungskontext unterscheidbar: Ein Diskussionsstrang beschäftigt sich mit evidenzinformierten Entscheidungen in der Bildungspolitik und der andere mit evidenzinformierten Entscheidungen und Handlungen in der Bildungspraxis. In beiden Diskussionen werden der Evidenz verschiedene Funktionen zugeschrieben. Bromme et al. [-@bromme2014b] etwa sprechen davon, dass Evidenz über Zustände informieren, Mechanismen erklären oder Interventionen evaluieren kann. Groß Ophoff et al. [@großophoff2023] wiederum unterscheiden konzeptuelle Nutzung (»*evidence allows focussing attention, provides new insights, challenges beliefs or reframes thinking*«, S. 02<!-- SB: hier jeweils 0 vor der 2 löschen oder ist das bewusst so?-->), instrumentelle Nutzung (»*identify or develop concrete measures to be taken*«, S. 02) und symbolische Nutzung (»*justif\[y\] or support of existing positions or established procedures*«, S. 02).<!-- SB: im Original nicht kursiv -> nur zitieren ohne kursiv oder die Kursivformatierung als eigene Hervorhebung kennzeichnen. Und inhaltich: soweit ich Janas Unterscheidung verstehe und auch den Kontext des Theorieteils des Papers, das Du zitierst, bezieht sich ihre Unterscheidung nur auf schulische Akteure, also Bildungspraxis, und nicht auf Bildungspolitik wie Bromme. So wie Bromme zu Beginn des Absatz eingeführt wird, liegt die Schlussfolgerung nahe, Janas Nutzungsformen auf Politik und Praxis zu beziehen. -->

## Potenzielle Wege zu einer gelingenden Wissenschaftskommunikation

Unabhängig vom Kontext und der Funktion evidenzinformierter Entscheidungen ist es plausibel anzunehmen, dass eine erfolgreiche Kommunikation im Sinne der Induktionen eines adäquaten Verständnisses von Evidenz zwischen Bildungswissenschaftler:innen/Fachdidaktiker:innen und den Akteuren im Bildungssystem eine notwendige Voraussetzung für das Gelingen evidenzinformierter Entscheidungen ist: Wird Evidenz fehlinterpretiert und erfolgt eine anschließende Entscheidung kohärent zu dieser Fehlinterpretation, wird die Wirkung dieser Entscheidung nicht die Erwünschte sein.

```{r}
#| label: fig-AbbildungMoMa
#| apa-twocolumn: true
#| fig-cap: Daten einer (fiktiven) Studie, eine dazugehörige Pressemitteilung und die Vorstellung einer Lehrkraft von den Daten # Daten der (fiktiven) Studie, Pressemitteilung und Vorstellung der Lehrkraft von den Daten
#| fig-height: 4
#| fig-width: 12
#| dev: "ragg_png"

library(tidyverse)
library(ggdist)
library(bayestestR)
library(hrbrthemes)
library(effectsize)
library(patchwork)
library(ggtext)
library(flextable)
library(geomtextpath)
library(colorspace)
library(brms)

set.seed(189)
data_reading_true <- 
    tibble(`Anzahl korrekt gelesener Worte pro Minute` = 
               round(c(rpois(500, 63), rpois(500, 61)), 1),
           Gruppe = c(rep("KI-Lesetutor", 500),
                      rep("Lautlesen", 500))) %>% 
    mutate(Gruppe = factor(Gruppe, 
                           levels = c("Lautlesen", "KI-Lesetutor")))

plot_true_data <- 
    ggplot(data_reading_true,
       aes(`Anzahl korrekt gelesener Worte pro Minute`, Gruppe)) +
    geom_dots(color = "#111111", fill = "#111111") +
    ylab("") +
    ggtitle("Daten", "der Studie") +
    theme_ipsum_rc()
        
plot_press <- 
    ggplot() +
    ggtitle("Ausschnitt", "der Pressemitteilung") +
    geom_richtext(
        data = data.frame(x = 1, y = 1, 
                          text = "In einer randomisierten Studie<br>mit *N* = 1001 Drittklässler:innen<br> zeigten diejenigen,<br>die täglich 15 Minuten<br> mit dem KI-Vorlesetutor übten,<br><b>signifikant bessere Leseflüssigkeit,</b><br>als Drittklässler:innen,<br>die täglich 15 Minuten<br>(ohne KI-Tutor) laut lasen."),
        aes(x = x, y = y, label = text),
        size = 3,
        label.color = "black") +
    theme_ipsum_rc() +
    theme(panel.background = element_blank(),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          axis.title.x = element_blank(),
          axis.title.y = element_blank(),
          axis.text.x = element_blank(),
          axis.text.y = element_blank())

data_teacher <- 
    tibble(`Anzahl korrekt gelesener Worte pro Minute` = 
               round(c(distribution_poisson(500, 70), 
                       distribution_poisson(500, 61)), 1),
           Gruppe = c(rep("KI-Lesetutor", 500),
                      rep("Lautlesen", 500))) %>% 
    mutate(Gruppe = factor(Gruppe, 
                           levels = c("Lautlesen", "KI-Lesetutor")))

plot_teacher_representation <- 
    ggplot(data_teacher,
       aes(`Anzahl korrekt gelesener Worte pro Minute`, Gruppe)) +
    geom_dots(color = "#111111", fill = "#111111") +
    ylab("") +
    ggtitle("Interpretation", "einer Lehrkraft") +
    theme_ipsum_rc() +
    theme(axis.text.y = element_blank())


plot_true_data + plot_press + plot_teacher_representation +
    plot_layout(guides = "collect")

#cliffs_delta(`Anzahl korrekt gelesener Worte pro Minute` ~ Gruppe,
#             data = data_reading_true)
#cliffs_delta(`Anzahl korrekt gelesener Worte pro Minute` ~ Gruppe,
#             data = data_teacher)
```

Liest eine Lehrkraft etwa die (fiktive) Pressemitteilung in @fig-AbbildungMoMa, stellt sich die Ergebnisse wie in @fig-AbbildungMoMa rechts vor [@schmidt2023] und überzeugt anschließend ihre Schulleitung, diesen KI-Lesetutor zu beschaffen und schulweit einzusetzen, liegt höchstwahrscheinlich dysfunktionales evidenzinformiertes Handeln vor. Während die Forscher:innen mit *signifikant bessere Leseflüssigkeit* zum Ausdruck bringen, dass ihre Daten unter der Annahme eines Nulleffekts unwahrscheinlich sind (signifikanter p-Wert), interpretiert die Lehrkraft diese Formulierung als »Unterschied **bedeutsamer Größe**«. Folglich schlussfolgert sie, dass es Sinn macht Geld und Zeit in Anschaffung und Implementation des KI-Lesetutors zu investieren,<!-- SB: einfügen: weil sie den KI-Lesetutor für deutlich lernwirksamer hält als lautes Lesen. und dann mit Allerdings und Hauptsatz weitermachen--> obwohl etwa die Implementation von Lesetandems kostengünstiger, weniger zeitaufwändig und lernwirksam gewesen wäre.<!-- SB: Mir unklar: meinst Du mit Lesetandems das gleiche oder was anderes als "lautlesen" im fiktiven Beispiel (es gibt auch Lautlesemethoden ohne Tandems.). Und als Kontext muss man wissen, was Lesetandems sind und dass sie lernwirksam sind, sonst ist der letzte Halbsatz eher verwirrend. Würde ich ggf. noch ergänzen, damit die Argumentation vollständig ist. Und unabhängig davon würde ich die Reihenfolge im letzten Satz umstellen: "Lesetandems lernwirksam und zudem kostengünstiger und weniger zeitaufwändig gewesen wäre. Denn es geht geht im fiktiven Bsp. ja erstmal um das Kriterium lernwirksamkeit (auch in der Studie) und dann nachgeordnet um Zeit und Kosten.  --> 

Die Forschung zur Wissenschaftskommunikation hat eine Reihe solcher potenziellen Problematiken aufgezeigt: Z.B. das soeben beschriebene Verwechseln von Inferenzstatistik und Effektstärke [@schmidt2023], das automatische Annehmen starker Effekte, wenn keine Effektstärken berichtet wurden [Practical Significance Bias, @michal2024], Rückschaufehler [@masnick2009] oder die verzerrte Einschätzung der Belastbarkeit von Befunden (z.B. das Ergebnis einer Laborstudie mit *N* = 56 mit großem Effekt und daher hoher statistischer Power) durch irrelevante Zahlen [z.B. Stichprobengröße einer zuvor gelesenen Large-Scale-Studie, @bohrer2025].

Gleichzeitig liegt eine Reihe von Befunden vor, die implizieren, dass verbesserte Kommunikation von Evidenz an Lehrkräfte zu Zwecken evidenzinformierten Handelns vergleichsweise einfach umsetzbar ist<!-- SB: Ich verstehe und stimme zu, aber das ist schon auch ein Claim, den man kurz ausführen oder belegen könnte :-) -->. Grundsätzlich lassen sich die bisherigen Befunde in angebotsseitige und nutzendenseitige Ansätze unterscheiden, also in Interventionen, die die Auswahl und Darstellung der Evidenz optimieren möchten und Ansätze, die bei der Scientific und Statistical Literacy der Lehrkräfte ansetzen [@bruhwiler2020].<!-- SB: auch hier könnte man seitenweise Begriffsklärung machen udn diskutieren, warum hier z.B. nicht data literacy kommt, obwohl unten die entsprechenden Beispielstudien kommen. Ist eine trade-off-Entscheidung, aber vielleicht ist es für distale Lesende einfacher, zu schreiben "Ansätze, die bei den Kompetenzen der Lehrpersonen ansetzen, also z.B. ihren Kompetenzen im Umgang mit Forschungsergebnissen oder Daten

Und noch zu der Unterscheidung Angebotsseitig - nutzendenseitig: in der DBDM Literatur gibt es die Unterscheidung konzeptuell, wenngleich nicht so explizit wie bei Brühweilers Rahmenmodell. Z.B. in dem ganz frühen Editorial Aufsatz von Kohler & Schrader (2004, inspiriert vom allgemeinen ANM) oder auch in dem Überblicksartikel von Altricher et al aus dem HB Neue Steuerung (dort eher feedbacktheoretisch begründet).   -->

Zu zweiterem gehören Programme wie »Data Teams« [@schildkamp2015]<!-- SB: hab mich hier über die Zitation gewundert. Wenn es um das data team procedure an sich geht, würde ich  Schildkamp, K., Handelzalts, A., Poortman, C. L., Leusink, H., Meerdink, M., Smit, M., Ebbeler, J., & Hubers, M. D. (2018). The Data Team™ Procedure: A Systematic Approach to School Improvement (K. Schildkamp, A. Handelzalts, C. L. Poortman, H. Leusink, M. Meerdink, M. Smit, J. Ebbeler, & M. D. Hubers, Eds.). Springer International Publishing. https://doi.org/10.1007/978-3-319-58853-7_9
zitieren, wenn um die Wirksamkeit, dann Poortman, C. L., & Schildkamp, K. (2016). Solving student achievement problems with a data use intervention for teachers. Teaching and Teacher Education, 60, 425–433. https://doi.org/10.1016/j.tate.2016.06.010
-->, welche durch ein umfängliches Set an vordefinierten Leitlinien und Aktivitäten versucht, konkrete schulische Probleme mit Hilfe von (oft eigens dafür genierten) Daten zu lösen, wobei meist 4-6 Lehrkräfte und Schulleiter:innen mit Bildungswissenschaftler:innen und Fachdidaktiker:innen kooperieren. Hierzu gehören auch »Brokering-Ansätze« (teilweise auch als »research practice partnerships« bezeichnet), in welchen Wissenschaftler:innen und Lehrpersonen (insbesondere Schulleitungen) gemeinsam versuchen, konkrete schulischen Probleme unter Rückgriff auf wissenschaftliche Erkenntnisse zu lösen [z.B. @sharples2015]. Auch Kurz- [@merk2020] oder längerfristig angelegte [@karst2024] Interventionen zur Anbahnung notwendiger Kompetenzen für evidenzinformiertes Handeln wie Graph Literacy<!-- SB: ggf. auch hier Begriff eher oder ergänzend umschreiben: , die Interpretation von grafisch dargestellten Daten --> [@friel2001] oder Forschungskompetenz [@neuenschwander2005] sowie die konkrete Unterstützung für solches<!-- SB: woraus bezieht sich solches? Syntaktisch ambig. besser: evidenzinformiertes Handeln nochmal schreiben und keinen Platzhalter --> [@zotero-8935]<!-- SB: hier ggf noch deutlich machen, dass es um Clearing house geht, denn das sieht man anhand der Zitation im Text nicht. -->, können diesem Ansatz zugerechnet werden.

Angebotsseitige Versuche die Kommunikation von Evidenz zu verbessern, stammen aus verschiedensten sozialwissenschaftlichen Disziplinen: So wird z.B. in der Psychologie<!-- SB: Ich war mal bei Tino in einem Vortrag, ob Psychologie eine Naturwissenschaft sei ... könnte mir vorstellen, dass manche Psycholog:innen hier an die Decke gehen, denn die Formulierung hier ordnet sie klar als Sozialwissenschaft ein ;-) --> untersucht, welche algebraisch äquivalenten Formulierungen zu standardisierten Effektstärken bei Rezeption durch Laien adäquatere Vorstellungen induzieren [siehe @tbl-wisskommbsp, z.B. @grice2020]. In der Human-Computer-Interaction-Forschung werden (teils dynamische) Visualisierungstechniken entwickelt, um Effektstärken und Inferential Uncertainty besser zu kommunizieren [z.B. @hullman2015; @zhang2023]. Die bildungswissenschaftliche Lehrerbildungsforschung sowie die Fachdidaktiken erproben innovative Formate für die Zielgruppe der Lehrkräfte [z.B. @schneider2024], was auch das Anliegen der vorliegenden Studie ist.

```{r}
#| label: tbl-wisskommbsp
#| tbl-cap: Beispiele für angebotsseitige Versuche verbesserter Kommunikation von Evidenz

library(gt)
library(timesaveR)
# Create table
tibble(`header1` = c("Standard-kommunikation", "Verbesserte Kommunikation"), 
       `Unterschied` = c("Die Leseleistung von Schülerinnen und Schülern in  (PISA 2022) sank um 28 Punkte und damit auf den Tiefststand.", "Die Leseleistungen von Schülerinnen und Schülern in Deutschland aus PISA 2018 und aus PISA 2022 überlappen sich zu 88,9%, wobei der Mittelwert um 28 Punkte sank."),
       `Zusammenhang` = c("Der sozioökonomische Status klärt 14% der Varianz der Mathematikleistung auf.", "Von 100 Schülerinnen und Schülern, die einen überdurchschnittlichen sozioökonomischen Status haben, zeigen 69 eine überdurchschnittliche Mathematikleistung.")) |>
  gt() %>% 
  gt_apa_style() %>% 
  cols_label(header1 = "") %>% 
  fmt_markdown(columns = everything()) %>% 
  opt_table_font(font = "Source Sans Pro")
    
```
<!-- SB: ich habe nicht verstanden, warum hier bei der Standardkommunikation (PISA 2022) in Klammern ist. Weil es exemplarisch gemeint ist?-->
# Die vorliegende Studie

In diesem Kontext untersucht die vorliegende Studie, inwiefern verbreitete Standardgrafiken zur Kommunikation der Entwicklung der Lesekompetenz in den deutschen Kohorten des Programme of International Student Assessment (PISA) Practical Significance Bias induzieren und ob dieser mit Grafiken eingedämmt werden kann<!-- SB: s.o.: verringert oder vermindert  -->, bei deren Gestaltung theoretische und empirische Erkenntnisse der Wissenschaftkommunikation [siehe Abschnitt @sec-materialien] berücksichtigt wurden.
<!-- SB: Ein etwas globalerer Kommentar: In der Studie wurden journalistische Grafiken verwendet und diese werden hier als "verbreitete Standardgrafiken" bezeichnet. Das suggeriert aus meiner Sicht an dieser Stelle auf der Basis des Theorieteils, wo es rein um Wissenschaftskommunikation durch Wissenschaft und gar nicht um Journalismus ging, dass es STandardgrafiken sind, die die Wissenschaft für die Wissenschaftskommunikation verwendet. Soweit ich das auf die Schnelle recherchieren konnte, gibt es solche Grafiken weder im allgemeinen OECD-Band noch in den entsprechenden deutschlandspezifischen Publikationen. Was es gibt, sind Liniendiagramme, wo jeweils Siginifikanz mit abgebildet ist (dotted vs durchgezogene Linien, aber auch keine Streuung oder Effektstärken) und diese gibt es auch z.B im IQB Bildungstrend (aber auch dort gibt es keine Liniendiagramme wie in Tab. 2. Dh, wenn man es genau nimmt, wird hier journalistische Wissenschaftskommunikation, die auf der Basis der Publikationen von Wissenschaftlichen Institutionen (wenn man die OECD so bezeichnen will;-)) herangezogen. Die Frage nach Stakeholdern/Akteursgruppen bzw. v.a. die Rolle des Journalismus wird in der Diskussion aber "nur" dichotom Wissenschaft - Praxis aufgegriffen. Man könnte ja gut argumentieren, dass die vorliegende Studie diese Einschränkung hat, aber dass es z.B. trotzdem relevant ist, weil a) die Originalgrafiken aus der Wissenschaft "nicht besser" sind und man davon ausgeht, dass Lehrkräfte damit in den Medien in Berührung kommen, daran evidenzorientiert handeln könnten und es deswegen probleamtisch und relevant ist o.ä.? -->
## Methode

### Materialien {#sec-materialien}

In der Berichterstattung zu den Ergebnissen der PISA-2022-Kohorte wurden durch journalistische Medien zahlreiche Darstellungsformate gewählt, insbesondere Liniendiagramme [siehe Tabelle @tbl-pisalinegraphs], was angesichts der Anlage des PISA als Trendstudie [@döring2016] konsequent erscheint.<!-- SB: Die Grafiken abzudrucken ist super, aber geht das vom Copyright her?-->

|  |  |  |
|------------------------|------------------------|------------------------|
| ![](img/sz.png){width="900"} | ![](img/tagessschau.jpg){width="900"} | ![](img/taz.jpeg){width="800"} |
| Süddeutsche Zeitung -@volkert2023 | RBB -@rbb2023 | taz -@taz.de2023 |

: Verwendete Liniendiagramme in der Berichterstattung. {#tbl-pisalinegraphs}

Diese Abbildungen erlauben einen effizienten Vergleich der Mittelwerte sowohl über die Zeit als auch Variablen (hier: Fächer) hinweg. In solchen Grafiken ist jedoch die Bedeutsamkeit der Mittelwertsdifferenz nur bei bekannter Streuung interpretierbar: @fig-mwdiffstreuung zeigt jeweils die gleichen Mittelwertsdifferenzen von 508<!-- SB: In den Grafiken stand MW = 580, ich glaube, das war ein Tippfehler; ich hab die Beschriftung im Code geändert --> (PISA Lesen 2015) und 480 (PISA Lesen 2022).

```{r}
#| label: fig-mwdiffstreuung
#| apa-twocolumn: true
#| fig-cap: Illustration der Unabhängigkeit von Mittelwertsdifferenz und Größe des Effekts
#| fig-height: 4
#| fig-width: 7
#| dev: "ragg_png"

mwdiffstreuungdata <- 
    tibble(Jahr = c(rep(2015, 100), rep(2022, 100),
                    rep(2015, 100), rep(2022, 100)),
           Streuung = c(rep("Kleine Streuung", 200),
                        rep("Reale Streuung", 200)),
           Lesen = c(distribution_normal(100, 508, 20),
                     distribution_normal(100, 480, 20),
                     distribution_normal(100, 508, 100),
                     distribution_normal(100, 480, 100)))

effsizes <-
    mwdiffstreuungdata %>%
    nest_by(Streuung) %>%
    summarize(
        cohd = cohens_d(Lesen ~ Jahr, data = data)$Cohens_d,
        overlap = 2 * pnorm(-abs(cohd) / 2) %>% round(.,2)
    )

# ggplot(mwdiffstreuungdata, aes(Jahr, Lesen, group = Jahr)) + 
#     ggforce::geom_sina(shape = 1) +
#     ggtitle("Gleiche Mittelwertsdifferenzen", 
#             "unterschiedliche Effektstärken") +
#     facet_wrap(~Streuung) + 
#     scale_x_continuous(breaks = c(2015, 2022)) +
#     theme_ipsum_rc()


ggplot(data.frame(x = c(0, 1000)), aes(x)) +
  geom_textline(
    stat = "function", 
    fun = dnorm,
    args = list(mean = 509, sd = 100),
    label = "MW = 508",
    size = 3, 
    fontface = 1, 
    hjust = .662, 
    vjust = 0,
    color = "purple"
  ) +
  stat_function(
    geom = "area", 
    fun = dnorm,
    args = list(mean = 509, sd = 100),
    fill = "#a01ff020",
    color="#ffffff00"
  ) +
  geom_textline(
    stat = "function", 
    fun = dnorm,
    args = list(mean = 480, sd = 100),
    label = "MW = 480",
    size = 3, 
    fontface = 1, 
    hjust = .331, 
    vjust = 0,
    color = "orange"
  ) +
  stat_function(
    geom = "area", 
    fun = dnorm,
    args = list(mean = 480, sd = 100),
    fill = "#ffa50020",
    color="#ffffff00"
  ) +
    annotate(
    "richtext", 
    x = 730, y = 0.0015, 
    label = "88% Über-<br>lappung", 
    hjust = 0, vjust = .5, 
    fill = NA, label.color = NA,
    size = 2.8
  ) +
    geom_curve(
    aes(x = 730, y = 0.0015, xend = 500, yend = 0.0012),
    curvature = 0.3, # Positive for upward curve, negative for downward
    arrow = arrow(length = unit(0.052, "inches"), type = "closed"), 
    linewidth = .1
  ) +
  xlab("") +
  ylab("") +
  theme_ipsum_rc() +
  theme(axis.text.y = element_blank()) +
ggplot(data.frame(x = c(400, 600)), aes(x)) +
  geom_textline(
    stat = "function", 
    fun = dnorm,
    args = list(mean = 509, sd = 20),
    label = "MW = 508",
    size = 3, 
    fontface = 1, 
    hjust = .676, 
    vjust = 0,
    color = "purple"
  ) +
  stat_function(
    geom = "area", 
    fun = dnorm,
    args = list(mean = 509, sd = 20),
    fill = "#a01ff020",
    color="#ffffff00"
  ) +
  geom_textline(
    stat = "function", 
    fun = dnorm,
    args = list(mean = 480, sd = 20),
    label = "MW = 480",
    size = 3, 
    fontface = 1, 
    hjust = .3, 
    vjust = 0,
    color = "orange"
  ) +
  stat_function(
    geom = "area", 
    fun = dnorm,
    args = list(mean = 480, sd = 20),
    fill = "#ffa50020",
    color="#ffffff00"
  ) +
    annotate(
    "richtext", 
    x = 550, y = 0.008, 
    label = "48% Über-<br>lappung", 
    hjust = 0, vjust = .5, 
    fill = NA, label.color = NA,
    size = 2.8
  ) +
    geom_curve(
    aes(x = 550, y = 0.008, xend = 500, yend = 0.006),
    curvature = 0.3, # Positive for upward curve, negative for downward
    arrow = arrow(length = unit(0.052, "inches"), type = "closed"), 
    linewidth = .1
  )+
  xlab("") +
  ylab("") +
  theme_ipsum_rc() + 
  theme(axis.text.y = element_blank())
```

Das Ausmaß der Bedeutsamkeit dieses (gleichen) Mittelwertsunterschiedes entsteht aber erst durch die Streuung der Daten um diesen Mittelwert herum. Weil die Variablen im rechten Teil der Abbildung kaum streuen, ist die Überlappung der beiden Gruppen gering (`{r} effsizes$overlap[1]*100`%, großer Effekt), während die große Überlappung im linken Teil (`{r} effsizes$overlap[2]*100`%, kleiner Effekt) durch die große Streuung zustande kommt. Die Abbildungen in [Tabelle @tbl-pisalinegraphs] sagen also nicht nur nichts über die Bedeutsamkeit der Mittelwertsunterschiede aus. Die nicht dargestellte Varianz induziert möglicherweise auch eine wahrgenommene große Bedeutsamkeit der Mittelwertsdifferenz [@kale2020].

|                                |                                        |
|--------------------------------|----------------------------------------|
| ![](img/taz.jpeg){width="400"} | ![](img/geomtextline.png){width="363"} |

: Verwendete Stimuli {#tbl-materials}

Daher wurden vorliegend neben Liniendiagrammen auch überlappende Verteilungskurven verwendet. Um diese barriereärmer zu gestalten wurde bei der Farbgebung auf hinreichenden Kontrast bei den prävalenten Sehbehinderungen geachtet [@garnier2023]. Um unnötige Arbeitsgedächtnisbelastung zu vermeiden, wurde die Legende direkt in die Grafik integriert [@franconeri2021].

### Design, Stichprobe und Instrument

```{r}
#| label: dataimport
data <- 
  read_csv("data/data_cummunication_PISA.csv") %>% 
  mutate(
      POS = case_when(
        is.na(G003_01) ~ G004_01,
        is.na(G004_01) ~ G003_01,
        is.na(G003_01) & is.na(G003_01) ~ NA),
      POS = ifelse(POS %in% c(-1,-9), NA, POS),
      POS0510 = POS/max(POS, na.rm = T)/2 + 0.5,
      Stimulus = as.factor(case_when(
        ZG01 == 1 ~ "Originalgrafik taz",
        ZG01 == 2 ~ "Überlappungsgrafik"))
  )
        
```
<!-- SB:Ich habe im folgenden für die beiden Grafiken/Stimuli konsequent die begriffe eingesetzt, die du hier als erstes im Absatz bzw. davor verwendet hast, also Liniendiagramm und überlappende Verteilungskurve. Habe das auch in den Grafiken geändert, damit alles zusammenpasst.  -->
In einem Between-Person Design wurde *N* = `{r} nrow(data)`<!-- SB:dieses N passt nicht mit dem in der Ergebnisgrafik zusammen (96+99) -> vermutlich wegen Missings/Ausschluss von Datenpunkten? --> Studierenden in Bachelorstudiengängen des Primar- und Sekundarstufenlehramtes randomisiert eine der beiden in @tbl-materials dargestellten Abbildungen gezeigt. Anschließend wurden sie mit folgenden Stimulus aufgefordert, die Effektstärke einzuschätzen: "*Basierend auf dieser Grafik: Wie hoch schätzen (exakte Antwort nicht möglich) Sie die Wahrscheinlichkeit ein, dass eine zufällig gezogene Schülerin oder ein zufällig gezogener Schüler aus dem Jahr 2022 im Lesen schlechter abschneidet als eine zufällig gezogene Schülerin oder Schüler aus dem Jahr 2015*?". Beantwortet wurde diese Frage mit einem Schieberegler, dessen Enden mit "*50% (beide Gruppen gleich)*" und "*100%* *(maximaler Effekt)*" beschriftet waren. Diese Erfassung der wahrgenommenen Effektstärke als »Probability of Superiority« ist in der Human-Computer-Interaction-Forschung verbreitet und gilt als valide [@brooks2014; @kim2022], wenngleich die Operationalisierung als Schieberegeler unklar lässt, inwiefern bei der Beantwortung tatsächlich eine Elaboration der Überlappung vorgenommen wird oder die Teilnehmenden eher wie bei einem Likert-Item vorgehen.<!-- Kirstin meinte, dass dieser Punkt näher ausgeführt werden oder gelöscht sollte : SB stimmt dem voll zu-->

### Statistische Analyse

Die abhängige Variable »Wahrgenommene Effektstärke« (operationalisiert als Probability of Superiority) ist per Design auf das geschlossene Intervall [0,5; 1] beschränkt und zeigt empirisch Bimodalität (siehe @fig-plotresults). Um diesen Umständen in der inferenzstatistischen Modellierung Rechnung zu tragen, wurden bayesianische Mixture Regressionsmodelle für zwei trunkierte Normalverteilungen [@frischkorn2023] in der probabilistischen Sprache Stan [@standevelopmentteam2024] mithilfe des R-Pakets {brms} [@bürkner2017] geschätzt.

```{r}
#| label: betareg
#| cache: false
#
# mod <- brm(
#     bf(POS0510 | trunc(lb = .5, ub = 1) ~ Stimulus),
#     data = data,
#     cores = 4,
#     iter = 100000,
#     seed = 5,
#     control = list(adapt_delta = 0.95),
#     family = mixture(gaussian(), gaussian()),
#     init = 0,
#     prior = c(
#         prior(normal(0.6, .6), Intercept, dpar = mu1),
#         prior(normal(.9, .6), Intercept, dpar = mu2)
#     )
# )
#save(mod, file = "data/mod.RData")
load("data/mod.RData")
hyp <- hypothesis(
  mod,
  "theta1 * b_mu1_StimulusÜberlappungsgrafik + (1-theta1) *
                         b_mu2_StimulusÜberlappungsgrafik < 0",
  class = NULL
)

manwhit <- wilcox.test(POS0510 ~ Stimulus, data = data)
cliffd <- cliffs_delta(POS0510 ~ Stimulus, data = data)$r_rank_biserial[1]
cohd <-  (2 * cliffd) / sqrt(1 - cliffd^2)
u3 <-  pnorm(cohd)
overlap <-  2*pnorm(-abs(cohd)/2)

# loop weniger gebiased effsize

ueberlappung_weniger_gebiased <- logical(0)

for(i in 1:10000){
  selected_original <- 
    data %>% 
    select(POS0510, Stimulus) %>% 
    filter(Stimulus == "Originalgrafik taz") %>% 
    na.omit() %>% 
    sample_n(1) %>% 
    pull(POS0510)
  
    selected_ueberlappung <- 
    data %>% 
    select(POS0510, Stimulus) %>% 
    filter(Stimulus == "Überlappungsgrafik") %>% 
    na.omit() %>% 
    sample_n(1) %>% 
    pull(POS0510)
    
    ueberlappung_weniger_gebiased[i] <- 
      abs(selected_ueberlappung - .58) < abs(selected_original - .58)
    
}
```

### Ergebnisse

Die Inspektion des Marcov-Chain-Monte-Carlo-Sampling-Prozesses zeigte eine zufriedenstellende Qualität bzgl. der Konvergenz $(\hat{R} < 1.01)$ und effektiver Sampling Size [$ESS_{Bulk} > 1000 < ESS_{Tail}$, @vehtari2021prefix].
<!-- SB: Zu der Frage ob erklären oder nicht: vielleicht nicht erklären, aber einen natürlichsprachlichen Satz einfügen, was es bedeutet a la Schätzung des Modells funktioniert anhand gängiger Indikatoren   -->
<!-- SB: Stimme dem Kommentar zu den Farben in der Ergebnisgrafik zu-->
```{r}
#| label: fig-plotresults
#| apa-twocolumn: true
#| fig-cap: Geschätze Effektstärke (Probability of Superiority) nach Stimulus. Beide Gruppen zeigen einen sehr deutlichen Practical Significance Bias (Abstand von Median und wahrem Wert).  ## ich habe mich gerade gefragt, ob es vielleicht sinnvoll ist die farbliche Encodierung in Abbildung 3 anders zu gestalten als in Tabelle 3.
#| fig-height: 4
#| fig-width: 9
#| dev: "ragg_png"

data_results <- data %>% 
  select(G003_01, G004_01) %>% 
  gather(Stimulus, `Probability of Superiority`, G003_01, G004_01) %>%
  filter(`Probability of Superiority` > 0) %>% 
  na.omit() %>% 
  mutate(
    Stimulus = case_when(
      Stimulus == "G003_01" ~ "Liniendiagramm",
      Stimulus == "G004_01" ~ "überlappende Verteilungskurve"
    ),
    `Probability of Superiority` = (`Probability of Superiority` - 50) /
      200 + .75)

pal <- c("#FF8C00", "#A034F0")

add_sample <- function(x) {
  return(c(y = max(x) + .025, 
           label = length(x)))
}
data_results |> 
  ggplot(aes(x = fct_rev(Stimulus), y = `Probability of Superiority`)) + 
    # add true value
    geom_hline(yintercept = .58) +
  ggdist::stat_halfeye(
    aes(color = Stimulus,
        fill = after_scale(lighten(color, .5))),
    adjust = .5, 
    width = .75, 
    .width = 0,
    justification = -.4, 
    point_color = NA
  ) +
  geom_boxplot(
    aes(color = stage(Stimulus, after_scale = darken(color, .1, space = "HLS")),
        fill = after_scale(desaturate(lighten(color, .8), .4))),
    width = .32, 
    outlier.shape = NA
  ) +
  geom_point(
    aes(color = stage(Stimulus, after_scale = darken(color, .1, space = "HLS"))),
    fill = "white",
    shape = 21,
    stroke = .4,
    size = 2,
    position = position_jitter(seed = 1, width = .12)
  ) + 
  geom_point(
    aes(fill = Stimulus),
    color = "transparent",
    shape = 21,
    stroke = .4,
    size = 2,
    alpha = .3,
    position = position_jitter(seed = 1, width = .12)
  ) + 
  stat_summary(
    geom = "text",
    fun = "median",
    aes(label = format(round(after_stat(y), 2), nsmall = 2),
        color = stage(Stimulus, after_scale = darken(color, .1, space = "HLS"))),
    family = "Roboto Mono",
    fontface = "bold",
    size = 4.5,
    vjust = -3.5
  ) +
  stat_summary(
    geom = "text",
    fun.data = add_sample,
    aes(label = paste("n =", after_stat(label)),
        color = stage(Stimulus, after_scale = darken(color, .1, space = "HLS"))),
    family = "Roboto Condensed",
    size = 4,
    hjust = 0
  ) +
  coord_flip(xlim = c(1.2, NA), clip = "off") +
  scale_color_manual(values = pal, guide = "none") +
  scale_fill_manual(values = pal, guide = "none") +
  labs(
    x = NULL,
    y = "Probability of Superiority"
  ) +
    
    # caption of true value
     
    annotate(
    "richtext", 
    y = .5, x = 2.9, 
    label = "wahrer<br>Wert", 
    hjust = 0, vjust = .5, 
    fill = NA, label.color = NA,
    size = 3.2
  ) +
    geom_curve(
    aes(y = .53, x = 2.8, yend = .566, xend = 2.6),
    curvature = 0.3, # Positive for upward curve, negative for downward
    arrow = arrow(length = unit(0.052, "inches"), type = "closed"), 
    linewidth = .1
  )+

  
  theme_minimal(base_family = "Roboto Condensed", base_size = 15) +
  theme(
    panel.grid.minor = element_blank(),
    panel.grid.major.y = element_blank(),
    axis.ticks = element_blank(),
    axis.text.x = element_text(family = "Roboto Mono"),
    axis.text.y = element_text(
      color = rev(darken(pal, .1, space = "HLS")), 
      size = 15
    ),
    axis.title.x = element_text(margin = margin(t = 10),
                                size = 16),
    plot.title = element_markdown(face = "bold", size = 21),
    plot.subtitle = element_text(
      color = "grey40", hjust = 0,
      margin = margin(0, 0, 20, 0)
    ),
    plot.title.position = "plot",
    plot.caption = element_markdown(
      color = "grey40", lineheight = 1.2,
      margin = margin(20, 0, 0, 0)),
    plot.margin = margin(15, 15, 10, 15)
  )
```

Die Medianeinschätzung der Probability of Superiority lag in beiden Gruppen deutlich über dem wahren Wert (Liniendiagramm .80, überlappende Überlappungsgrafik .73). Dieser Unterschied in der Einschätzung entspricht einer Überlappung von `{r} round(overlap*100, 2)`% (Cliff's *d* = `{r} round(cliffd, 2)`) oder anders ausgedrückt: Legt man 100 mal einem:einer Studierenden die Originalgrafik und einem:einer Studierenden die Überlappungsgrafik vor, schätzt `{r} round(mean(ueberlappung_weniger_gebiased),2)*100`mal die:der Studierende mit der Überlappungsgrafik den Effekt weniger verzerrt ein. Die Inferenzstatistik für diesen Unterschied ist mit einer Evidence Ratio von `{r} round(hyp$hypothesis$Evid.Ratio, 1)` klar konklusiv: Die vorliegenden Daten sind 14,8-fach wahrscheinlicher unter der Annahme, dass der Mittelwert in der geschätzten Effektstärke in der Gruppe mit der überlappenden Verteilungskurve niedriger ist als unter der Annahme, dass beide gleich sind.

# Diskussion

Der vorliegende Beitrag zielt darauf ab, zu eruieren, inwiefern es nach dem Stand der Forschung gestaltete Wissenschaftskommunikation ermöglicht, bildungswissenschaftliche und fachdidaktische Evidenz »besser« an Lehrkräfte zu kommunizieren. Dabei wurde »besser« als »weniger gebiased« operationalisiert und gezeigt, dass die Wahl einer theoretisch fundierten grafischen Darstellung einen deutlich geringeren Bias induzierte als eine Standardgrafik. Allerdings war auch die Rezeption der verbesserten Darstellung immer noch erheblich verzerrt (siehe @fig-plotresults).

Im Lichte dieser Ergebnisse werden im Folgenden drei Implikationen diskutiert: 1) Die Forderung, dass Lehrkäfte ihre professionelle Praxis evidenzinformiert gestalten sollen, setzt Anstrengungen in der Wissenschaftskommunikation seitens Bildungswissenschaften und Fachdidaktiken voraus. 2) Inwiefern diese Anstrengungen zielführend sind, sollte empirisch überprüft werden. 3) Erfolgreiche Wissenschaftskommunikation in den Bildungswissenschaften und Fachdidaktiken impliziert eine Passung von Angebots- und Nutzendenmerkmalen und damit einen dialogischen Prozess für die Entwicklung einer solchen Passung.
<!-- Ich habe im folgenden Formulierungen eingefügt, die deutlich machen sollen, wo es jetzt nicht um "innerwissenschaftliche" Wissenschaftskommunikation geht sondern um Wissenschaft - Praxis. Das ist zwar eigentlich klar, aber für mich war es an manchen Stellen nicht klar genug (z.B. in dem Teil mit und damit vielelicht für distalere Lesende auch nicht (?)-->
Sowohl Wissenschaftstheorie [z.B. @mitchell2010] als auch bildungswissenschaftliche Literatur [z.B. @bohl2015a; @dewe1992]<!-- SB: hier passen html und qmd nciht zusammen: bohl erscheint nicht und ich hab noch ein z.B. ergänzt vor Bohl, weil es ja exemplarische Zitationen sind--> haben die Möglichkeiten und Limitationen der Abgrenzbarkeit von »Wissenschaft« und »Nicht-Wissenschaft« (bzw. in den Bildungswissenschaften von »Theorie« und »Praxis«) diskutiert und heben u.a. hervor, dass Entitäten und Aussagen in ihrer Bedeutung primär an den Herkunftskontext (z.B. »Wissenschaft« oder »Praxis«) gebunden sind. Also ist auch z.B. die »Evidenz« einer explanativen bildungswissenschaftlichen Studie per se zunächst bildungswissenschaftlich und muss für eine evidenzinformierte Handlung in der Praxis reinterpretiert werden [z.B. @großophoff2023]. Damit liegt es auf der Hand, dass sich Bildungswissenschaftler:innen und Fachdidaktiker:innen fragen <!-- sollten - das ist ja der claim und tun sie ja nicht immer-->, welche »wissenschaftlichen/theoretischen« Entitäten (z.B. Effektstärken oder inferenzstatistischen Maße) und Aussagen (z.B. kausale Effekte) sie wie in die Kommunikation ihrer Ergebnisse <!-- gegenüber der Praxis--> aufnehmen.

Dass diese Forderung<!-- selbst rein innerwissenschaftlich betrachtet--> nicht trivial ist, zeigt z.B. die Tatsache, dass Guidelines von Fachgesellschaften wie z.B. der American Psychological Association<!-- hier American Psychological Association ausschreiben und dann Zitierung ohne Author, denn das ist im html aktuell komisch und APA anhand der Zitation nicht erkennbar--> [-@association2019] die Verwendung von Effektstärken fordern, diese aber in Pressemitteilungen (etwa der American Educational Research Association) und selbst in Fachzeitschriften selten sind, obwohl die Fachgesellschaften bzw. Zeitschriften diese in ihren Autor:innenrichtlinien verbindlich fordern [@mcmillan2011]. <!-- Für die Wissenschaftskommunkation für die Praxis und dann Es raus--> Es scheint es also plausibel, zu schlussfolgern, dass es unter Forschenden noch nicht verbreitet scheint, sich literaturbasiert darüber Gedanken zu machen, inwiefern die eigene Wissenschaftskommunikation <!-- z.B. für Lehrkräfte--> günstig rezipierbar ist.

Doch selbst ein Bewusstsein für die Fallstricke der Kommunikation wissenschaftlicher Ergebnisse schützt nicht zwangläufig vor der Induktion von Fehlvorstellungen: So fanden Schneider et al. -@schneider2024 etwa, dass selbst eine als leicht verständlich geltende Effektstärke für Mittelwertsvergleiche (wie etwa Cohen's $U_3$) bei einem erheblichen Anteil <!-- hier und bei den Cohens u3 klammer davor passt was nicht mit den Klammern im html(?)-->(≥ 29%) der Rezipient:innen zu Fehlvorstellungen führte. Die erste Implikation <!-- , Anstrengungen der Wissneschaft bei der Wissenschaftskommunikation für die Praxis-->, scheint also nicht hinreichend für eine gelingende Kommunikation von Evidenz an Lehrkräfte. Dies führt zur zweiten Implikation: Forschende sollten nicht nur den Stand der Forschung bei der Kommunikation von Evidenz berücksichtigen, sondern auch in intern und extern validen Studien untersuchen, inwieweit diese Berücksichtigung erfolgreich war. Denn statistische Informationen werden nicht nur von unterschiedlichen Berufsgruppen [@mcdowell2017], sondern auch in unterschiedlichen geografischen Regionen differentiell interpretiert [@gigerenzer2005]. Inwiefern sich also generische Determinaten erfolgreicher Wissenschaftskommunikation auf die Kommunikation von Evidenz an Lehrkräfte <!-- etwa--> in einem bestimmten Teil eines Bildungssystems übertragen lassen, scheint nur schwer a priori bestimmbar.

Was aber, wenn Forschende ihre Wissenschaftskommunikation literaturbasiert verbessern, aber in empirischen Experimenten sehen, dass sie dennoch deutlich verzerrt, verrauscht oder konzeptuell falsch rezipiert wird? Der vorliegende Beitrag macht <!-- hier als dritte Implikation--> den Vorschlag, den Prozess der Kommunikation von Evidenz an Lehrkräfte als dialogisch aufzufassen und zu berücksichtigen, dass bei der Rezeption von Wissenschaftskommunikation vermutlich eine komplexe Interaktion von Angebots- und Nutzendenmerkmalen [@bruhwiler2020] und Bottom-Up- bzw. Top-Down-Prozessen [@schmidt2024] vorliegt.<!-- Im Theorieteil und am Beginn der Diskussion wird nur die Angebots-Nutzungs-Unterscheidung eingeführt und erklärt (und hier wird aber nun von einer komplexen Interaktion gesprochen), aber Bottom up/top down kommt erst hier und wird eher nur gedroppt und nicht eingeführt bzw. von ANM unterschieden. Das macht das Verstehen und die Kohärenz vor allem für distale Lesende aus meienr Sicht echt schwer, weil ja beide Rahmungen zwar konzeptuell Überschneidungen haben, aber nicht synonym/deckungsgleich sind. Deshalb a) nur ein Rahmenmodell einführen und im kompletten Artikel verwenden (dann vielleicht eher nur ANM)? b) beide durchgängig verwenden? c) bottom up/top down hier einführen und dann erklären, worin die komplexe Interaktion besteht?-->Um es an einem Beispiel zu illustrieren: Stellt man sich eine Lehrkraft vor, die auf der Suche nach einer Entscheidungsgrundlage für oder gegen eine unterrichtsgestalterische Maßnahme A auf der Seite eines Clearing Houses landet. Dort liest sie, dass über viele Studien gemittelt Maßnahme A dazu geführt hat, dass 63% der Schülerinnen und Schüler bessere Leistungen zeigen als der Mittelwert der Schülerinnen und Schüler mit Maßnahme B. Dann können daraus manche Lehrkräfte <!-- möglicherweise (das ist ja eine Behauptung hier)--> anhand ihres Vorwissens unmittelbar eine korrekte/konsistente Vorstellung der Effektstärke des Unterschieds *schlussfolgern* (z.B. zwei Normalverteilungen mit 87% Überlappung). Hier läge ein Top-Down-Prozess vor, da die Merkmale der Kommunikation mit den im Langzeitgedächtnis der Rezipient:in vorhandenen Dispositionen<!--im html steht Kapazitäten - wie kommt das zustande? Bin ich wieder im falschen file?--> wie Graph oder Statistical Literacy dazu führen, dass in einem Schlussfolgerungsprozess ein korrektes mentales Modell erstellt wird. Umgekehrt kann es passieren, dass eine Lehrkraft auf diese Formulierung stößt und eben kein auf Wissen basierendes mentales Modell abrufen kann - aber sich Stück für Stück mithilfe der gegebenen Informationen ein konsistentes mentales Modell erarbeitet. Dabei *lernt* sie, d.h. erwirbt Statistical Literacy oder Graph Literacy, was einem Bottom-Up Prozess entspricht. Da Lehrkräfte über sehr unterschiedliche Kapazitäten zu Top-Down-Prozessen verfügen, aber auch Bottom-Up-Prozesse sehr individuell verlaufen dürften, liegt die dritte Implikation nahe: Die Kommunikation von Evidenz an Lehrkräfte sollte als dialogischer Prozess<!--und differenziell, oder? Diese Schlussfolgerung liegt für mich total nahe anhand der Argumentation im gesamten Diskussionsteil, also dass one size fits all eben nicht passt sondern das ganze differenziell betrachtet werden müsste. --> aufgefasst werden. Demnach würden zum einen Bildungswissenschaftler:innen und Fachdidaktiker:innen Kenntnis über Top-Down- und Bottom-Up-Prozesse ihrer Rezipienten:innen erwerben und deren <!--Ausprägung und-->Entwicklung z.B. anhand von Think-Aloud-Studien wie z.B. @bez2021 beobachten <!--und daraufhin ihre Angebote entsprechend differenzieren und anpassen-->. Zum anderen könnten Lehrkräfte in die Entwicklung von Kommunikationsprodukten anhand kokonstruktiver Verfahren eingebunden werden, in der Hoffnung, dass eine solche Kooperation von Akteuren aus den Systemen »Wissenschaft/Theorie« und »Nicht-Wissenschaft/Praxis« dazu führt, dass innerhalb dieser Systeme Ausdrucksweisen verfügbar werden, die zu verlustfreieren und damit erfolgreicheren Kommunikationsprozessen führen können [@leitz2024]<!--ich kenne leitz nicht und wollte es nachschauen - aber das erscheint nicht im Literaturverzeichnis (?)-->.

## Literatur
