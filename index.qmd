---
title: Evidenz.Besser.Kommunizieren.
subtitle: Wie Bildungswissenschaften und Fachdidaktiken ihre Wissenschaftskommunikation weiterentwickeln können.
shorttitle: Evidenz.Besser.Kommunizieren.
author:
  - name: Samuel Merk
    corresponding: true
    orcid: 0000-0003-2594-5337
    email: merk@ph-karlsruhe.de
    roles:
      - conceptualization
      - data curation
      - formal Analysis
      - investigation
      - methodology
      - software
      - supervision
      - validation
      - visualization
      - writing inital draft
      - editing
    affiliations:
      - id: id1
        name: "Pädagogische Hochschule Karlsruhe"
        address: Bismarckstraße 10
        city: Karlsruhe
        country: Germany
        postal-code: 76133
  - name: Sarah Bez
    affiliations:
      - id: id1
  - name: Kirstin Schmidt
    affiliations:
      - id: id1
author-note:
  disclosures:
    data-sharing: Die Daten dieses Artikels sind zu finden unter
    conflict-of-interest: Die Autor\:innen haben keine Interessenskonflikte zu berichten
abstract: "Lehrkräfte treffen tagtäglich unzählige Entscheidungen bzgl. ihrer Unterrichtsgestaltung und -entwicklung. Dabei rekurrieren Sie vornehmlich auf persönliche Erfahrung, Konzeptwissen oder Heuristiken. Evidenz aus Bildungswissenschaften und Fachdidaktiken wird das Potenzial zugeschrieben diese Entscheidungsprozesse komplementär zu informieren und zu objektivieren. Dazu ist es jedoch notwendig, dass die betroffenen Lehrkräfte diese Evidenz nicht fehlinterpretieren, was wiederum entsprechende Kompetenzen der Lehrkräfte oder besonders geschickte <!-- gelungene?--> Wissenschaftkommunikation voraussetzt. Der vorliegende Beitrag untersucht daher die Möglichkeiten und Grenzen der Kommunikation von Effektstärken an Lehramtsstudierende am Beispiel des sog. zweiten PISA-Schocks. Im Ergebniss zeigt sich, dass Lehramtsstudierende Effektstärken sehr ungenau (Noise) ein- und im Mittel drastisch überschätzen (Practical Significance Bias). Dieser Bias konnte durch die Verwendung alternativer Visualisierungen deutlich eingedämmt werden $(d = .5)$"
# Put as many keywords at you like, separated by commmas (e.g., [reliability, validity, generalizability])
keywords: [Lehrpersonenprofessionalisierung, Wissenschaftskommunikation, Practical Significance Bias]
# If true, tables and figures are mingled with the text instead of listed at the end of the document.
floatsintext: true
# Numbered lines (.pdf and .docx only)
numbered-lines: false
# File with references
bibliography: references.bib
# Suppress title page
suppress-title-page: false
# Link citations to references
link-citations: true
notebook-links: true
lang: de
---

<!-- Max. 35k inklusive allem-->

Die bildungswissenschaftliche Literatur zu Schul- und Unterrichtsentwicklung bedient sich einer Vielzahl theoretischer Grundlegungen [@bohl2020] und blickt daher aus ganz verschiedenen Winkeln auf diesen Gegenstand: Neben eher systemtheoretischen Perspektiven [@bauer1978] finden sich u.a. Ansätze mit Entlehnungen aus der Lehr- Lern- [@helmke2022] und Organisationspsychologie [@holtappels2007] oder <!-- "mit" ergänzen? --> Praxisorientierung als Leitgedanke [@bruegelmann2018]. Datenbasierte Schulentwicklung hat im deutschsprachigen Raum <!-- eher - löschen? --> erst in den vergangenen zwei Dekaden Momentum gefunden, wenngleich deren Grundidee des empirischen Einholens von Information über den Ist-Stand schon zuvor gefordert und auch umgesetzt wurde [@altrichter2006]. In jüngerer Zeit ist jedoch von inner- wie außerwissenschaftlichen Stakeholdern vermehrt die Forderung nach einer Entwicklung von Schule und Unterricht hörbar geworden, die ihre Entscheidungen durch Evidenz informiert [@aero2023; @bauer2012; @eurlex2024; @pellegrini2021; @slavin2020]. Da jedoch einerseits weder die Genese noch die Interpretation von Evidenz zu den professionellen Kernkompetenzen von Lehrkäften zählt, <!-- Da jedoch einerseits die Genese und Interpretation von Evidenz nicht zu den professionellen Kernkompetenzen von Lehrkäften gehört,--> andererseits Bildungswissenschaftler- und Fachdidaktiker:innen keine Expert:innen für die Gestaltung von Unterricht sind, plädiert der vorliegende Beitrag dafür, Wissenschaftskommunikation erstens als dialogischen Prozess zwischen Bildungswissenschaften/Fachdidaktiken und Lehrkräften aufzufassen <!-- darauf bist du im Theorieteil nicht näher eingegangen; kommt aber ja vielleicht in der Diskussion? --> und zweitens diesen forschungsbasiert weiter zu entwickeln.

<!-- Die Einführung von datenbasierter und evidenzinformierter SE ist vom Aufbaue sehr ähnlich... kann man das vielleicht zusammen nehmen z.B. unter systematisch gewonnene Infos als Entscheidungsgrundlage oder ähnliches? -->

Daher führt der folgende theoretische Hintergrund zunächst in Konzepte und Begriffe evidenzinformierter Praxis ein, bevor er auf Wissenschaftskommunikation in Bildungswissenschaften und Fachdidaktiken eingeht, um abschließend ein empirisches Beispiel zu skizzieren.


# Theoretischer Hintergrund

## Evidenzinformiertes Handeln

### Was kann unter »Evidenz« verstanden werden?

Ethymologisch kann »Evidenz« als Substantivierung des Adjektivs »evident« gesehen werden [@kluge2011, S.263], welches wiederum im 18. Jahrhundert dem lateinischen »evidens« [»ersichtlich, augenscheinlich«, @hau2012] entlehnt wurde [@stark2017]. Allerdings meinen Bildungswissenschaftler:innen und Fachdidaktiker:innen gerade nicht »das Augenscheinliche« oder »das direkt Ersichtliche« wenn sie von Evidenz sprechen - vielmehr ist in Definitionsvorschlägen von »wissenschaftlichem Wissen« [@stark2017], von einer »Funktion« von Daten für die Bestätigung oder Widerlegung von Hypothesen und Theorien [@bromme2014b] oder <!-- "von" ergänzen? --> »warrants for making assertions or knowledge claims« [@shavelson2002] die Rede. In einer aktuellen Systematisierung verschiedener Verständnisse des Evidenz-Begriffs <!-- in den Bildungswissenschaften ergänzen oder denkst du/ihr das wird durch den Satz zuvor ausreichend deutlich --> hebt Schmidt [-@schmidt2024] hervor, dass nur wenige Definitionen ausschließlich quantitativer Empirie die Möglichkeit zuschreiben, Evidenz zu generieren, sondern meistens auch qualitative Empirie, Theorien sowie mathematische und logische Analysen <!-- was genau zählst du zu mathematischen und logischen Analysen? --> als potentiell <!-- Duden empfielt die Schreibweise: potenziell --> evidenzgenerierend definiert werden. Insbesondere die Inklusion nicht-empirischer Entitäten wie »Theorien« oder »logische Analysen« mögen auf den ersten Blick widersprüchlich wirken, da der Begriff Evidenz insbesondere im deutschsprachigen Raum eher mit Ergebnissen explanativer quantitativer Studien assoziiert scheint. <!-- ist das wirklich so? kommen die meisten Vorschläge Evidenz breiter zu fassen nicht eher aus dem deutschsprachigen Raum? Ich denke beispielsweise an Stark, Bauer und Kollar--> Dieser scheinbare Widerspruch wirkt jedoch weniger stark, berücksichtigt man, dass insbesondere in der Lehr- Lernforschung mit »Theorien« wohl eher sogenannte »tried-and-tested theories« [@renkl2022] gemeint sein dürften. Diese stellen eher Rahmenmodelle oder sogenannte »interventional models« (z.B. Cognitive Theory of Multi-Media Learning) dar (ebd.), die wiederum meist stark von empirischen Ergebnissen beeinflusst sind. Daher ist es plausibel »Theorien« die Funktion als »warrant« für »knowledge claims« zuzuschreiben - sie also auch als »Evidenz« zu bezeichnen. <!-- Diese stellen eher Rahmenmodelle oder sogenannte »interventional models« (z.B. Cognitive Theory of Multi-Media Learning) dar (ebd.). Da solche »Theorien« wiederum meist stark von empirischen Ergebnissen beeinflusst sind, ist es plausibel ihnen die Funktion als »warrant« für »knowledge claims« zuzuschreiben - sie also auch als »Evidenz« zu bezeichnen. -->

### Evidenzbasiert, evidenzinformiert, evidenzorientiert.

<!-- Evidenzinformiert, evidenzorientiert, evidenzbasiert. vielleicht umstellen, dass es zur Kapitelstruktur passt ? --> 

Im vorigen Abschnitt wurde deutlich, dass »Evidenz« ein uneinheitlich gebrauchter und gleichermaßen komplex wie unscharf definierter Begriff ist. Im Lichte dessen erscheint es nur konsequent, dass auch die Begriffe evidenzbasiert, evidenzinformiert, evidenzorientiert, e datenbasiert, forschungsbasiert und forschungsinformiert klassisches *Jingle and Jangle* [@thorndike1904; @kelly2023] darstellen - hier also unterschiedliche Begriffe für das Gleiche und gleiche Begriffe für Unterschiedliches gebraucht werden. Die Differenzen zwischen evidenz**basiert** und evidenz**informiert** sowie evidenz**orientiert** <!-- korrekterweise zwischen evidenzbasiert und evidenzinformiert sowie evidenzorientiert, oder --> können jedoch auch bedeutsam interpretiert werden: Da mit »Evidenz**basierung**« oft »the medical model« [@jones2024] und damit Evidenz aus *Kontrollgruppenexperimenten* als *notwendige Voraussetzung* für eine Entscheidung assoziiert wird, zieht dieser Begriff die stärkste Kritik auf sich [@bellmann2011; @biesta2007]. Den Begriffen »evidenz**orientiert**« und »evidenz**informiert**« wird mit weniger Fundamentalkritik begegnet, da diese schon rein sprachlich eher eine heuristische denn eine rechenschaftlegende Rolle impliziert. <!-- m.E. muss dieser Punkt noch etwas ausgeführt werden, damit man ihn versteht. Mir ist beispielsweise nicht ganz klar, was du mit rechenschaftslegend wirklich meinst (und gibt es diesen Begriff in dieser Form überhaupt?--> 

In der deutschsprachigen bildungswissenschaftlichen Diskussion sind nach Bromme et al.[-@bromme2014b] zunächst zwei verschiedene Diskussionsstränge bzgl. evidenzinformierter Entscheidungen im Bildungskontext zu differenzieren<!-- unterscheidbar -->: Einer <!-- klingt etwas umgangssprachlich, oder? Vielleicht Ein Diskussionsstrang--> beschäftigt sich mit evidenzinformierten Entscheidungen in der Bildungspolitik und einer <!-- der andere --> mit evidenzinformierten Entscheidungen und Handlungen in der Bildungspraxis. In beiden Diskussionen werden der Evidenz verschiedene Funktionen zugeschrieben. Bromme et al. [-@bromme2014b] etwa sprechen davon, dass Evidenz über Zustände informieren, Mechanismen erklären oder Interventionen evaluieren kann. Groß Ophoff et al. [@großophoff2023] wiederum unterscheiden konzeptuelle Nutzung (»*evidence allows focussing attention, provides new insights, challenges beliefs or reframes thinking*«, S. 02), instrumentelle Nutzung (»*identify or develop concrete measures to be taken*«, S. 02) und symbolische Nutzung (»*justif\[y\] or support of existing positions or established procedures*«, S. 02).

## Potenzielle Wege zu einer gelingenden Wissenschaftskommunikation

<!-- ich stolpere beim Lesen etwas über die Struktur des Kapitels v.a. ab dem Punkt, ab dem du auf die Angebots- und Nutzendenseite eingehst.  Nach meiner Lesart starten wir mit der Angebotsseite und problematischen Angeboten, dann wird darüber informiert, dass es eine Angebots- und Nutzendenseite gibt und der Vorteil der Angebotsseite in der leichteren Umsetzbarkeit liegt, aber an den genannten Beispiele ist vermutlich nur mit Vorwissen die leichtere Umsetzbarkeit zu erkennen (oder sollen vielleicht gar nicht den Vorteil hervorheben). Vielleicht verstehe ich es aber auch falsch...
Daher mein Vorschlag: Macht es Sinn das Kapitel mit der notwendigen Voraussetzung zu starten, dann den Abschnitt zu Angebot und Nutzung, dann dein KI-Beispiel und die weiteren Problematiken in der Wissenschaftskommunikationsforschung und dann den Vorteil der leichteren Umsetzbarkeit und die bisher verwendeten Forschungsansätze --> 


Unabhängig vom Kontext und der Funktion evidenzinformierter Entscheidungen ist es plausibel anzunehmen, dass eine erfolgreiche Kommunikation von Evidenz zwischen Bildungswissenschaftler:innen/Fachdidaktiker:innen und den Akteuren im Bildungssystem notwendige Voraussetzung für das Gelingen evidenzinformierter Entscheidungen ist  <!-- ich weiß wir haben darüber schon öfter darüber nachgedacht, aber wir verwenden es (glaube ich) nicht konsistent: Was ist die notwendige Bedingung oder Voraussetzung? Ist es wirklich die erfolgreiche Kommunikation oder ist es nicht viel eher die erfolgreiche Interpretation der Evidenz wofür eine erfolgreiche Wissenschaftskommunikation ein Katalysator sein kann. Gerade wenn man an Angebot- und Nutzung denkt, finde ich letzteres überzeugender. Würden wir nur Ansätze verfolgen, die die Lehrpersonen mit hoher scientific und statistical literacy ausstatten, wäre es egal wie schlecht das Angebot ist, oder?--> : Wird Evidenz fehlinterpretiert und erfolgt eine anschließende Entscheidung kohärent zu dieser Fehlinterpretation wird die Wirkung dieser Entscheidung nicht die Erwünschte sein.

```{r}
#| label: fig-AbbildungMoMa
#| apa-twocolumn: true
#| fig-cap: Daten einer (fiktiven) Studie, eine dazugehörige Pressemitteilung und die Vorstellung einer Lehkraft von den Daten # Daten der (fiktiven) Studie, Pressemitteilung und Vorstellung der Lehkraft von den Daten
#| fig-height: 4
#| fig-width: 12
#| dev: "ragg_png"

library(tidyverse)
library(ggdist)
library(bayestestR)
library(hrbrthemes)
library(effectsize)
library(patchwork)
library(ggtext)
library(flextable)
library(geomtextpath)
library(colorspace)
library(brms)

set.seed(189)
data_reading_true <- 
    tibble(`Anzahl korrekt gelesener Worte pro Minute` = 
               round(c(rpois(500, 63), rpois(500, 61)), 1),
           Gruppe = c(rep("KI-Lesetutor", 500),
                      rep("Lautlesen", 500))) %>% 
    mutate(Gruppe = factor(Gruppe, 
                           levels = c("Lautlesen", "KI-Lesetutor")))

plot_true_data <- 
    ggplot(data_reading_true,
       aes(`Anzahl korrekt gelesener Worte pro Minute`, Gruppe)) +
    geom_dots(color = "#111111", fill = "#111111") +
    ylab("") +
    ggtitle("Daten", "der Studie") +
    theme_ipsum_rc()
        
plot_press <- 
    ggplot() +
    ggtitle("Ausschnitt", "der Pressemitteilung") +
    geom_richtext(
        data = data.frame(x = 1, y = 1, 
                          text = "In einer randomisierten Studie<br>mit *N* = 1001 Drittklässler:innen,<br> zeigten diejenigen,<br>die täglich 15 Minuten<br> mit dem KI-Vorlesetutor übten,<br><b>signifikant bessere Leseflüssigkeit,</b><br>als Drittklässler:innen,<br>die täglich 15 Minuten<br>(ohne KI-Tutor) laut lasen."),
        aes(x = x, y = y, label = text),
        size = 3,
        label.color = "black") +
    theme_ipsum_rc() +
    theme(panel.background = element_blank(),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          axis.title.x = element_blank(),
          axis.title.y = element_blank(),
          axis.text.x = element_blank(),
          axis.text.y = element_blank())

data_teacher <- 
    tibble(`Anzahl korrekt gelesener Worte pro Minute` = 
               round(c(distribution_poisson(500, 70), 
                       distribution_poisson(500, 61)), 1),
           Gruppe = c(rep("KI-Lesetutor", 500),
                      rep("Lautlesen", 500))) %>% 
    mutate(Gruppe = factor(Gruppe, 
                           levels = c("Lautlesen", "KI-Lesetutor")))

plot_teacher_representation <- 
    ggplot(data_teacher,
       aes(`Anzahl korrekt gelesener Worte pro Minute`, Gruppe)) +
    geom_dots(color = "#111111", fill = "#111111") +
    ylab("") +
    ggtitle("Interpretation", "einer Lehrkraft") +
    theme_ipsum_rc() +
    theme(axis.text.y = element_blank())


plot_true_data + plot_press + plot_teacher_representation +
    plot_layout(guides = "collect")

#cliffs_delta(`Anzahl korrekt gelesener Worte pro Minute` ~ Gruppe,
#             data = data_reading_true)
#cliffs_delta(`Anzahl korrekt gelesener Worte pro Minute` ~ Gruppe,
#             data = data_teacher)

    
```

Liest eine Lehrkraft etwa die (fiktive) Pressemitteilung in @fig-AbbildungMoMa, stellt sich die Ergebnisse wie in @fig-AbbildungMoMa rechts vor [@schmidt2023] und überzeugt anschließend ihre Schulleitung diesen KI-Lesetutor zu beschaffen und schulweit einzusetzen liegt höchstwahrscheinlich dysfunktionales evidenzinformiertes Handeln vor. <!-- deine ursprüngliche Formulierung: Denn die Forscher:innen bringen mit *signifikant bessere Leseflüssigkeit* zum Ausdruck, dass ihre Daten unter der Annahme eines Nulleffekts unwahrscheinlich sind (signifikanter p-Wert). Die Lehrkraft, jedoch interpretiert diese Formulierung als »Unterschied bedeutsamer Größe«. Folglich schlussfolgert sie, dass es Sinn macht Geld und Zeit in Anschaffung und Implementation des KI-Lesetutors zu investieren obwohl etwa die Implementation von Lesetandems [@ehlert2024] kostengünstiger, weniger zeitaufwändig und lernwirksamer gewesen wäre. --> Während die Forscher:innen mit *signifikant bessere Leseflüssigkeit* zum Ausdruck bringen, dass ihre Daten unter der Annahme eines Nulleffekts unwahrscheinlich sind (signifikanter p-Wert), interpretiert die Lehrkraft diese Formulierung als »Unterschied bedeutsamer Größe«. Folglich schlussfolgert sie, dass es Sinn macht Geld und Zeit in Anschaffung und Implementation des KI-Lesetutors zu investieren obwohl etwa die Implementation von Lesetandems [@ehlert2024] kostengünstiger, weniger zeitaufwändig und lernwirksamer gewesen wäre. <!-- auch wenn wir hier ein fiktives Beispiel beschreiben - können wir aus nicht-fiktive Studien, dass Lerntandems effektiver sind als Lesetutors? Falls nein oder unklar, würde ich folgendes schriebe: "obwohl etwa die Implementation von Lesetandems kostengünstiger, weniger zeitaufwändig und lernwirksam gewesen wäre" --> 


Die Forschung zur Wissenschaftskommunikation hat eine Reihe solcher potenziellen Problematiken aufgezeigt: Z.B. das soeben beschriebene Verwechseln von Inferenzstatistik und Effektstärke [@schmidt2023], das automatische Annehmen starker Effekte, wenn keine Effektstärken berichtet wurden [Practical Significance Bias, @michal2024], Rückschaufehler [@masnick2009] oder die verzerrte Einschätzung der Belastbarkeit von Befunden (z.B. Ergebnis einer Laborstudie mit *N* = 56 mit großem Effekt und daher hoher statistischer Power) durch irrelevante Zahlen [z.B. Stichprobengröße einer zuvor gelesenen Large-Scale-Studie, @bohrer2025].

Gleichzeitig liegt eine Reihe von Befunden vor, die implizieren, dass verbesserte Kommunikation von Evidenz an Lehrkräfte zu Zwecken evidenzinformierten Handelns vergleichsweise einfach umsetzbar ist. Grundsätzlich lassen sich die bisherigen Befunde in angebotsseitige und nutzendenseitige Ansätze unterscheiden, also in Interventionen, die die Auswahl und Darstellung der Evidenz optimieren möchten und Ansätze, die bei der Scientific und Statistical Literacy der Lehrkräfte ansetzen.
<!-- ursprüngliche Formulierung: Gleichzeitig liegt eine Reihe von Befunden vor, die implizieren, dass verbesserte Kommunikation von Evidenz an Lehrkräfte zu Zwecken evidenzinformierten Handelns vergleichsweise einfach umsetzbar ist. Diese lassen sich zunächst in angebotsseitige und nutzendenseitige Ansätze unterscheiden, also in Interventionen, die die Auswahl und Darstellung der Evidenz optimieren möchten und Ansätze, die bei der Scientific und Statistical Literacy der Lehrkräfte ansetzen.  

Kommentar: m.E. ist der erste Satz etwas irreführend, denn man könnte diesen Abschnitt so lesen, dass Angebots- und Nutzungsseite der verbesserten Kommunikation von Evidenz zuordnen lassen. --> 

::: callout-caution
Gibt es diese Unterscheidung auch in der Literatur oder nur in unseren Gesprächen?

Antwort: Einen ersten Ansatz findest du bei Brühwiler et al (2020). Das Modell ist zwar sehr umfassen und m.E. auch nicht ideal, aber als Referenz ein erster guter Ansatz. 
Brühwiler, C., & Leutwyler, B. (2020). Praxisrelevanz von Forschung als gemeinsame Aufgabe von Wissenschaft und Praxis: Entwurf eines Angebots-Nutzungs-Modells. BzL - Beiträge zur Lehrerinnen- und Lehrerbildung, 38(1), 21–36. https://doi.org/10.36950/bzl.38.2020.9309 

man könnte im weiteren Sinne auch auf Debiasing-Forschung verweisen, aber glaube die Referenz von Brühwiler passt am besten
:::

Zu zweiterem gehören Programme wie »Data Teams« [@schildkamp2015], welche durch ein umfängliches Set an vordefinierten Leitlinien und Aktivitäten versucht, konkrete schulische Probleme mit Hilfe von (oft eigens dafür genierten) Daten zu lösen, wobei meist 4-6 Lehrkräfte und Schulleiter:innen mit Bildungswissenschaftler:innen und Fachdidaktiker:innen kooperieren. <!-- es gibt auch research-practice partnerships die ähnlich angelegt sind, falls du das aufnehmen willst, sag gerne Bescheid, dann ergänze ich die Quelle--> Auch kurz- [@merk2020] oder längerfristig angelegte [@karst2024] Interventionen zur Anbahnung notwendiger Kompetenzen für evidenzinformiertes Handeln wie Graph Literacy [@friel2001] oder Forschungskompetenz [@neuenschwander2005] sowie die konkrete Unterstützung für solches [@zotero-8935], können diesem Ansatz zugerechnet werden.

Angebotsseitige Versuche die Kommunikation von Evidenz zu verbessern, stammen aus verschiedensten sozialwissenschaftlichen Disziplinen: So wird z.B. in der Psychologie untersucht, welche algebraisch äquivalenten Formulierungen zu standardisierten Effektstärken bei Rezeption durch Laien adäquatere Vorstellungen induzieren [siehe @tbl-wisskommbsp, z.B. @grice2020]. In der Human Computer Interaction Forschung werden (teils dynamische) Visualisierungstechniken entwickelt, um Effektstärken und Inferential Uncertainty besser zu kommunizieren [z.B. @hullman2015, @zhang2023] <!-- hier stimmt in der Anzeige etwas nicht (z.B. wird Hullman nicht angezeigt)--> und die bildungswissenschaftliche Lehrerbildungsforschung sowie die Fachdidaktiken erproben innovative Formate für die Zielgruppe der Lehrkräfte [z.B. @schneider2024], was auch das Anliegen der vorliegenden Studie ist.

```{r}
#| label: tbl-wisskommbsp
#| tbl-cap: Beispiele für angebotsseitige Versuche verbesserter Kommunikation von Evidenz

library(gt)
library(timesaveR)
# Create table
tibble(`header1` = c("Standard-kommunikation", "Verbesserte Kommunikation"), 
       `Unterschied` = c("Die Leseleistung von Schülerinnen und Schülern in  (PISA2022) sank um 28 Punkte und damit auf den Tiefststand.", "Die Leseleistungen von Schülerinnen und Schülern in Deutschland aus PISA2018 und aus PISA2022 überlappen sich zu 88,9% wobei der Mittelwert um 28 Punkte sank."),
       `Zusammenhang` = c("Der sozioökonomische Status klärt 14% der Varianz der Mathematikleistung auf.", "Von 100 Schülerinnen und Schülern, die einen überdurchschnittlichen sozioökonomischen Status haben, zeigen 69 eine überdurchschnittliche Mathematikleistung.")) |>
  gt() %>% 
  gt_apa_style() %>% 
  cols_label(header1 = "") %>% 
  fmt_markdown(columns = everything()) %>% 
  opt_table_font(font = "Source Sans Pro")
    
```

# Die vorliegende Studie

In diesem Kontext untersucht die vorliegende Studie, inwiefern verbreitete Standardgrafiken zur Kommunikation der Entwicklung der Lesekompetenz in den deutschen Kohorten des Programme of International Student Assessment (PISA) Practical Significance Bias induzieren und ob dieser mit Grafiken eingedämmt werden kann, bei deren Gestaltung theoretische und empirische Erkenntnisse der Wissenschaftkommunikation [siehe Abschnitt @sec-materialien] berücksichtigt wurden.

## Methode

### Materialien {#sec-materialien}

In der Berichterstattung zu den Ergebnissen der PISA2022-Kohorte wurden durch journalistische Medien zahlreiche Darstellungsformate gewählt, insbesondere Line Graphs <!-- ist es üblich im Deutschen den englischen Begriff zu verwenden? --> [siehe Tabelle @tbl-pisalinegraphs], was angesichts der Anlage des PISA als Trendstudie [@döring2016] konsequent erscheint.

|  |  |  |
|----|----|----|
| ![](img/sz.png){width="900"} | ![](img/tagessschau.jpg){width="900"} | ![](img/taz.jpeg){width="800"} |
| Süddeutsche Zeitung -@volkert2023 | RBB -@rbb2023 | taz -@taz.de2023 |

: Verwendete Line Graphs in der Berichterstattung. {#tbl-pisalinegraphs}

Diese Abbildungen erlauben einen effizienten Vergleich der Mittelwerte sowohl über die Zeit als auch Variablen (hier: Fächer) hinweg. In solchen Grafiken ist jedoch die Bedeutsamkeit der Mittelwertsdifferenz nur bei bekannter Streuung interpretierbar <!-- ich finde die Formulierung etwas misleading. Denn die Differenz der Mittelwerte (im Sinne einer Effektstärke (nicht standardisierte Effektstärke) ist ja auch interpretierbar, aber nicht ausreichend um angemessene Rückschlüsse zu ziehen o.ä. (wie du es ja dann auch im nächsten Abschnitt schreibst>). Daher habe ich Bedeutsamkeit eingefügt, da du den Begriff unten auch verwendest-->: @fig-mwdiffstreuung zeigt jeweils die gleichen Mittelwertsdifferenzen von 508 (PISA Lesen 2015) und 480 (PISA Lesen 2022).

```{r}
#| label: fig-mwdiffstreuung
#| apa-twocolumn: true
#| fig-cap: Illustration der Unabhängigkeit von Mittelwertsdifferenz und Größe des Effekts
#| fig-height: 4
#| fig-width: 7
#| dev: "ragg_png"

mwdiffstreuungdata <- 
    tibble(Jahr = c(rep(2015, 100), rep(2022, 100),
                    rep(2015, 100), rep(2022, 100)),
           Streuung = c(rep("Kleine Streuung", 200),
                        rep("Reale Streuung", 200)),
           Lesen = c(distribution_normal(100, 508, 20),
                     distribution_normal(100, 480, 20),
                     distribution_normal(100, 508, 100),
                     distribution_normal(100, 480, 100)))

effsizes <-
    mwdiffstreuungdata %>%
    nest_by(Streuung) %>%
    summarize(
        cohd = cohens_d(Lesen ~ Jahr, data = data)$Cohens_d,
        overlap = 2 * pnorm(-abs(cohd) / 2) %>% round(.,2)
    )

# ggplot(mwdiffstreuungdata, aes(Jahr, Lesen, group = Jahr)) + 
#     ggforce::geom_sina(shape = 1) +
#     ggtitle("Gleiche Mittelwertsdifferenzen", 
#             "unterschiedliche Effektstärken") +
#     facet_wrap(~Streuung) + 
#     scale_x_continuous(breaks = c(2015, 2022)) +
#     theme_ipsum_rc()


ggplot(data.frame(x = c(0, 1000)), aes(x)) +
  geom_textline(
    stat = "function", 
    fun = dnorm,
    args = list(mean = 509, sd = 100),
    label = "MW = 580",
    size = 3, 
    fontface = 1, 
    hjust = .662, 
    vjust = 0,
    color = "purple"
  ) +
  stat_function(
    geom = "area", 
    fun = dnorm,
    args = list(mean = 509, sd = 100),
    fill = "#a01ff020",
    color="#ffffff00"
  ) +
  geom_textline(
    stat = "function", 
    fun = dnorm,
    args = list(mean = 480, sd = 100),
    label = "MW = 480",
    size = 3, 
    fontface = 1, 
    hjust = .331, 
    vjust = 0,
    color = "orange"
  ) +
  stat_function(
    geom = "area", 
    fun = dnorm,
    args = list(mean = 480, sd = 100),
    fill = "#ffa50020",
    color="#ffffff00"
  ) +
    annotate(
    "richtext", 
    x = 730, y = 0.0015, 
    label = "88% Über-<br>lappung", 
    hjust = 0, vjust = .5, 
    fill = NA, label.color = NA,
    size = 2.8
  ) +
    geom_curve(
    aes(x = 730, y = 0.0015, xend = 500, yend = 0.0012),
    curvature = 0.3, # Positive for upward curve, negative for downward
    arrow = arrow(length = unit(0.052, "inches"), type = "closed"), 
    linewidth = .1
  ) +
  xlab("") +
  ylab("") +
  theme_ipsum_rc() +
  theme(axis.text.y = element_blank()) +
ggplot(data.frame(x = c(400, 600)), aes(x)) +
  geom_textline(
    stat = "function", 
    fun = dnorm,
    args = list(mean = 509, sd = 20),
    label = "MW = 580",
    size = 3, 
    fontface = 1, 
    hjust = .676, 
    vjust = 0,
    color = "purple"
  ) +
  stat_function(
    geom = "area", 
    fun = dnorm,
    args = list(mean = 509, sd = 20),
    fill = "#a01ff020",
    color="#ffffff00"
  ) +
  geom_textline(
    stat = "function", 
    fun = dnorm,
    args = list(mean = 480, sd = 20),
    label = "MW = 480",
    size = 3, 
    fontface = 1, 
    hjust = .3, 
    vjust = 0,
    color = "orange"
  ) +
  stat_function(
    geom = "area", 
    fun = dnorm,
    args = list(mean = 480, sd = 20),
    fill = "#ffa50020",
    color="#ffffff00"
  ) +
    annotate(
    "richtext", 
    x = 550, y = 0.008, 
    label = "48% Über-<br>lappung", 
    hjust = 0, vjust = .5, 
    fill = NA, label.color = NA,
    size = 2.8
  ) +
    geom_curve(
    aes(x = 550, y = 0.008, xend = 500, yend = 0.006),
    curvature = 0.3, # Positive for upward curve, negative for downward
    arrow = arrow(length = unit(0.052, "inches"), type = "closed"), 
    linewidth = .1
  )+
  xlab("") +
  ylab("") +
  theme_ipsum_rc() + 
  theme(axis.text.y = element_blank())
```

Das Ausmaß der Bedeutsamkeit dieses (gleichen) Mittelwertsunterschiedes entsteht aber erst durch die Streuung der Daten um diesen Mittelwert herum. Dadurch dass die Variablen im linken Teil der Abbildung kaum streuen, ist die Überlappung der beiden Gruppen gering (`{r} effsizes$overlap[1]*100`%, großer Effekt), während die große Überlappung im rechten Teil (`{r} effsizes$overlap[2]*100`%, kleiner Effekt) durch die große Streuung zustande kommt. <!-- aktuell sind die Grafiken umgekehrt abgebildet, daher entweder im Text oder in der Abbildung ändern. Ich habe es nicht im Text geändert, da ich mir nicht sicher war, was dir lieber ist -->  Die Abbildungen in [Tabelle @tbl-pisalinegraphs] sagen also nicht nur nichts über die Bedeutsamkeit der Mittelwertsunterschiede aus - die nicht dargestellte Varianz induziert womöglich <!-- womöglicherweise womöglich oder möglicherweise :) --> auch eine wahrgenommene große Bedeutsamkeit der Mittelwertsdifferenz [@kale2020].

|                                |                                        |
|--------------------------------|----------------------------------------|
| ![](img/taz.jpeg){width="400"} | ![](img/geomtextline.png){width="363"} |

: Verwendete Stimuli {#tbl-materials}

Daher wurden vorliegend neben Line Graphs auch überlappende Verteilungskurven verwendet. Um diese barriereärmer zu gestalten wurde bei der Farbgebung auf hinreichenden Kontrast bei den prävalenten Sehbehinderungen geachtet [@garnier2023]. Außerdem wurde die Legende direkt in die Grafik integriert mit dem Ziel unnötige Arbeitsgedächtnisbelastung zu vermeiden <!-- Um unnötige Arbeitsgedächtnisbelastung zu vermeiden wurde die Legende direkt in die Grafik integriert-->  [@franconeri2021].

### Design, Stichprobe und Instrument

```{r}
#| label: dataimport
data <- 
  read_csv("data/data_cummunication_PISA.csv") %>% 
  mutate(
      POS = case_when(
        is.na(G003_01) ~ G004_01,
        is.na(G004_01) ~ G003_01,
        is.na(G003_01) & is.na(G003_01) ~ NA),
      POS = ifelse(POS %in% c(-1,-9), NA, POS),
      POS0510 = POS/max(POS, na.rm = T)/2 + 0.5,
      Stimulus = as.factor(case_when(
        ZG01 == 1 ~ "Originalgrafik taz",
        ZG01 == 2 ~ "Überlappungsgrafik"))
  )
        
```

In einem <!-- einfachen - würde ich löschen --> Between-Person Design wurde *N* = `{r} nrow(data)` Studierenden in Bachelorstudiengängen des Primar- und Sekundarstufenlehramtes randomisiert entweder der Line-Graph oder die überlappende Verteilungskurve (vgl. @tbl-materials ) <!-- eine der beiden in @tbl-materials dargestellten Abbildungen--> gezeigt. Anschließend wurden sie mit folgenden Stimulus aufgefordert die Effektstärke einzuschätzen: "*Basierend auf dieser Grafik: Wie hoch schätzen (exakte Antwort nicht möglich) Sie die Wahrscheinlichkeit ein, dass eine zufällig gezogene Schülerin oder ein zufällig gezogener Schüler aus dem Jahr 2022 im Lesen schlechter abschneidet als eine zufällig gezogene Schülerin oder Schüler aus dem Jahr 2015*". Beantwortet wurde diese Frage mit einem Schieberegler, dessen Enden <!-- wie folgt beschriftet waren:--> mit "*50% (beide Gruppen gleich)*" und "*100%* *(maximaler Effekt)*" beschriftet waren. Diese Erfassung der wahrgenommenen Effektstärke als »Probability of Superiority« ist in der Human Computer Interaction Forschung verbreitet und gilt als valide [@brooks2014; @kim2022], wenngleich die Operationalisierung als Schieberegeler unklar lässt, inwiefern bei der Beantwortung tatsächlich eine Elaboration der Überlappung vorgenommen wird oder die Beantwortenden <!-- besser Teilnehmende? --> eher wie bei einem Likert-Item vorgehen. <!-- m.E. müsste dieser Punkt näher ausgeführt werden oder gelöscht... so finde ich ihn ohne Zusatzinfos schwer nachvollziehbar -->

### Statistische Analyse

Die abhängige Variable »Wahrgenommene Effektstärke« (operationalisiert als Probability of Superiority) ist per Design auf das geschlossene Intervall [0,5; 1] beschränkt und zeigt empirisch Bimodalität (siehe @fig-plotresults). Um diesen Umständen in der inferenzstatistischen Modellierung Rechnung zu tragen, wurden bayesianische mixture Regressionsmodelle <!-- schriebt man die Begriffe nicht groß? also Bayesianische Mixture Regressionsmodelle --> für zwei trunkierte Normalverteilungen [@frischkorn2023] in der probabilistischen Sprache Stan [@standevelopmentteam2024] mithilfe des R-Pakets {brms} [@bürkner2017] geschätzt.

```{r}
#| label: betareg
#| cache: true
#
# mod <- brm(
#     bf(POS0510 | trunc(lb = .5, ub = 1) ~ Stimulus),
#     data = data,
#     cores = 4,
#     iter = 100000,
#     seed = 5,
#     control = list(adapt_delta = 0.95),
#     family = mixture(gaussian(), gaussian()),
#     init = 0,
#     prior = c(
#         prior(normal(0.6, .6), Intercept, dpar = mu1),
#         prior(normal(.9, .6), Intercept, dpar = mu2)
#     )
# )
#save(mod, file = "data/mod.RData")
load("data/mod.RData")
hyp <- hypothesis(
  mod,
  "theta1 * b_mu1_StimulusÜberlappungsgrafik + (1-theta1) *
                         b_mu2_StimulusÜberlappungsgrafik < 0",
  class = NULL
)

manwhit <- wilcox.test(POS0510 ~ Stimulus, data = data)
cliffd <- cliffs_delta(POS0510 ~ Stimulus, data = data)$r_rank_biserial[1]
cohd <-  (2 * cliffd) / sqrt(1 - cliffd^2)
u3 <-  pnorm(cohd)
overlap <-  2*pnorm(-abs(cohd)/2)

# loop weniger gebiased effsize

ueberlappung_weniger_gebiased <- logical(0)

for(i in 1:10000){
  selected_original <- 
    data %>% 
    select(POS0510, Stimulus) %>% 
    filter(Stimulus == "Originalgrafik taz") %>% 
    na.omit() %>% 
    sample_n(1) %>% 
    pull(POS0510)
  
    selected_ueberlappung <- 
    data %>% 
    select(POS0510, Stimulus) %>% 
    filter(Stimulus == "Überlappungsgrafik") %>% 
    na.omit() %>% 
    sample_n(1) %>% 
    pull(POS0510)
    
    ueberlappung_weniger_gebiased[i] <- 
      abs(selected_ueberlappung - .58) < abs(selected_original - .58)
    
}
```

### Ergebnisse

Die Inspektion des Marcov-Chain-Monte-Carlo-Sampling-Prozesses zeigte eine zufriedenstellende Qualität bzgl. der Konvergenz $(\hat{R} < 1.01)$ und effektiver Sampling Size [$ESS_{Bulk} > 1000 < ESS_{Tail}$, @vehtari2021prefix].  <!-- versteht man das ohne Vorwissen? --> 

```{r}
#| label: fig-plotresults
#| apa-twocolumn: true
#| fig-cap: Geschätze Effektstärke (Probability of Superiority) nach Stimulus. Beide Gruppen zeigen einen sehr deutlichen Practical Significance Bias (Abstand von Median und wahrem Wert).  ## ich habe mich gerade gefragt, ob es vielleicht sinnvoll ist die farbliche Encodierung in Abbildung 3 anders zu gestalten als in Tabelle 3.
#| fig-height: 4
#| fig-width: 9
#| dev: "ragg_png"

data_results <- data %>% 
  select(G003_01, G004_01) %>% 
  gather(Stimulus, `Probability of Superiority`, G003_01, G004_01) %>%
  filter(`Probability of Superiority` > 0) %>% 
  na.omit() %>% 
  mutate(
    Stimulus = case_when(
      Stimulus == "G003_01" ~ "Originalgrafik taz",
      Stimulus == "G004_01" ~ "Überlappungsgrafik"
    ),
    `Probability of Superiority` = (`Probability of Superiority` - 50) /
      200 + .75)

pal <- c("#FF8C00", "#A034F0")

add_sample <- function(x) {
  return(c(y = max(x) + .025, 
           label = length(x)))
}
data_results |> 
  ggplot(aes(x = fct_rev(Stimulus), y = `Probability of Superiority`)) + 
    # add true value
    geom_hline(yintercept = .58) +
  ggdist::stat_halfeye(
    aes(color = Stimulus,
        fill = after_scale(lighten(color, .5))),
    adjust = .5, 
    width = .75, 
    .width = 0,
    justification = -.4, 
    point_color = NA
  ) +
  geom_boxplot(
    aes(color = stage(Stimulus, after_scale = darken(color, .1, space = "HLS")),
        fill = after_scale(desaturate(lighten(color, .8), .4))),
    width = .32, 
    outlier.shape = NA
  ) +
  geom_point(
    aes(color = stage(Stimulus, after_scale = darken(color, .1, space = "HLS"))),
    fill = "white",
    shape = 21,
    stroke = .4,
    size = 2,
    position = position_jitter(seed = 1, width = .12)
  ) + 
  geom_point(
    aes(fill = Stimulus),
    color = "transparent",
    shape = 21,
    stroke = .4,
    size = 2,
    alpha = .3,
    position = position_jitter(seed = 1, width = .12)
  ) + 
  stat_summary(
    geom = "text",
    fun = "median",
    aes(label = format(round(after_stat(y), 2), nsmall = 2),
        color = stage(Stimulus, after_scale = darken(color, .1, space = "HLS"))),
    family = "Roboto Mono",
    fontface = "bold",
    size = 4.5,
    vjust = -3.5
  ) +
  stat_summary(
    geom = "text",
    fun.data = add_sample,
    aes(label = paste("n =", after_stat(label)),
        color = stage(Stimulus, after_scale = darken(color, .1, space = "HLS"))),
    family = "Roboto Condensed",
    size = 4,
    hjust = 0
  ) +
  coord_flip(xlim = c(1.2, NA), clip = "off") +
  scale_color_manual(values = pal, guide = "none") +
  scale_fill_manual(values = pal, guide = "none") +
  labs(
    x = NULL,
    y = "Probability of Superiority"
  ) +
    
    # caption of true value
     
    annotate(
    "richtext", 
    y = .5, x = 2.9, 
    label = "wahrer<br>Wert", 
    hjust = 0, vjust = .5, 
    fill = NA, label.color = NA,
    size = 3.2
  ) +
    geom_curve(
    aes(y = .53, x = 2.8, yend = .566, xend = 2.6),
    curvature = 0.3, # Positive for upward curve, negative for downward
    arrow = arrow(length = unit(0.052, "inches"), type = "closed"), 
    linewidth = .1
  )+

  
  theme_minimal(base_family = "Roboto Condensed", base_size = 15) +
  theme(
    panel.grid.minor = element_blank(),
    panel.grid.major.y = element_blank(),
    axis.ticks = element_blank(),
    axis.text.x = element_text(family = "Roboto Mono"),
    axis.text.y = element_text(
      color = rev(darken(pal, .1, space = "HLS")), 
      size = 15
    ),
    axis.title.x = element_text(margin = margin(t = 10),
                                size = 16),
    plot.title = element_markdown(face = "bold", size = 21),
    plot.subtitle = element_text(
      color = "grey40", hjust = 0,
      margin = margin(0, 0, 20, 0)
    ),
    plot.title.position = "plot",
    plot.caption = element_markdown(
      color = "grey40", lineheight = 1.2,
      margin = margin(20, 0, 0, 0)),
    plot.margin = margin(15, 15, 10, 15)
  )
```

<!-- ich finde es wichtig hier noch zu beschreiben, dass der Median beider Einschätzungen über dem wahren Wert liegt --> 

Die Medianeinschätzung der Probability of Superiority betrug .80 (Originalgrafik) bzw. .73 (Überlappungsgrafik). <!-- Dieser Unterschied in der Einschätzung --> Der Unterschied in diesen Einschätzungen <!-- So ist es sprachlich passender zum vorherigen Satz, oder? --> entspricht einer Überlappung von `{r} round(overlap*100, 2)`% (Cliff's *d* = `{r} round(cliffd)`) oder anders ausgedrückt: Legt man 100 mal einem:einer Studierenden die Originalgrafik und einem:einer Studierenden die Überlappungsgrafik vor, schätzt `{r} round(mean(ueberlappung_weniger_gebiased),2)*100`mal die:der Studierende mit der Überlappungsgrafik den Effekt weniger verzerrt ein. Die Inferenzstatistik für diesen Unterschied ist mit einer Evidence Ratio von `{r} round(hyp$hypothesis$Evid.Ratio, 1)` klar konklusiv: Die vorliegenden Daten sind 14,8-fach wahrscheinlicher unter der Annahme, dass der Mittelwert in der geschätzten Effektstärke in der Gruppe mit der Überlappungsgrafik niedriger ist als unter der Annahme, dass beide gleich sind.

# Diskussion

## Literatur
